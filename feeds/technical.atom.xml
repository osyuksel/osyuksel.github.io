<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ömer Yüksel - Technical</title><link href="https://osyuksel.github.io/" rel="alternate"></link><link href="https://osyuksel.github.io/feeds/technical.atom.xml" rel="self"></link><id>https://osyuksel.github.io/</id><updated>2025-06-15T14:00:00+02:00</updated><entry><title>The color memory test: a quick filter for language model quality</title><link href="https://osyuksel.github.io/blog/llm-color-test/" rel="alternate"></link><published>2025-06-14T20:00:00+02:00</published><updated>2025-06-15T14:00:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-06-14:/blog/llm-color-test/</id><summary type="html">&lt;p&gt;A simple yet effective test for weeding out unreliable LLMs using eye and hair color descriptions.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Language model benchmarks typically test various capabilities like mathematical reasoning, coding, and world knowledge.
While valuable for ranking models, these comprehensive tests can be overwhelming for quick reliability assessments.&lt;/p&gt;
&lt;p&gt;In my experience working with LLMs, I've found that sometimes what we need is a simple binary criterion – a basic test that can help us quickly reject unreliable models. One such test I've been using evaluates a fundamental capability that any reliable LLM should possess: the ability to overcome statistical biases in training data when presented with explicit contradicting information.&lt;/p&gt;
&lt;h3 id="the-eye-and-hair-color-test"&gt;The Eye and Hair Color Test&lt;/h3&gt;
&lt;p&gt;The test is deceptively simple: we present the model with descriptions of people having unusual eye and hair colors, then ask it to recall these attributes. Here's an example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ali has pink eyes and blue hair. John has red eyes and green hair. Karl has copper eyes and violet hair. Lucy has green eyes and brown hair. Dimitri has brown eyes and black hair.&lt;/p&gt;
&lt;p&gt;Given the above information, answer the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is Ali's eye color?&lt;/li&gt;
&lt;li&gt;What is John's hair color?&lt;/li&gt;
&lt;li&gt;What is Karl's hair color?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;What makes this test particularly interesting is that it challenges two common biases in LLM training data. First,
there's statistical prevalence: most descriptions of people in training data would contain common eye colors (brown,
blue, green) and hair colors (black, brown, blonde). Second (related but different), there's real-world knowledge: the model knows that humans typically don't have pink eyes or blue hair.&lt;/p&gt;
&lt;p&gt;A reliable model should be able to override both these biases and simply repeat what it was told in the prompt. After all, if a model can't accurately recall explicitly stated information because it's "unusual", how can we trust it with more complex reasoning tasks?&lt;/p&gt;
&lt;h3 id="why-this-test-matters"&gt;Why This Test Matters&lt;/h3&gt;
&lt;p&gt;This test is particularly relevant in the context of LLM development history. Early models (pre-&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback"&gt;RLHF&lt;/a&gt; era) often struggled with this type of task, showing a strong tendency to "hallucinate" more common colors despite explicit contradicting information in the prompt. The model would effectively override the given information with what it considered more "likely" or "realistic" based on its training data.&lt;/p&gt;
&lt;p&gt;In the current landscape, this issue is largely solved in major models like GPT-4o, Claude 4, or Gemini 2.5, but remains a useful discriminator for evaluating smaller, local models. When working with lightweight or fine-tuned models, this test can quickly reveal if the model has maintained the fundamental ability to override its statistical biases or if compression and optimization have compromised this capability.&lt;/p&gt;
&lt;p&gt;This behavior indicates a fundamental reliability issue. If a model can't override its statistical biases for simple
color recall, it likely struggles with more complex scenarios where correct answers contradict its training data
patterns.&lt;/p&gt;
&lt;h3 id="beyond-simple-recall"&gt;Beyond Simple Recall&lt;/h3&gt;
&lt;p&gt;The test, seemingly about memory, actually evaluates several important capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Information precedence&lt;/strong&gt;: Can the model prioritize explicitly given information over its statistical priors?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual override&lt;/strong&gt;: Can it temporarily suspend its world knowledge when presented with a scenario that contradicts it?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instruction following&lt;/strong&gt;: Can it stick to simply reporting what it was told without embellishment or "correction"?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These capabilities are fundamental to many practical applications. For instance, if you're using an LLM for processing technical documentation, you want it to faithfully maintain the specific details provided, regardless of what it has been trained with. This becomes especially crucial when working with newer versions of libraries or frameworks. If the model keeps defaulting to its training data instead of the current documentation, you'll end up fighting against its outdated assumptions rather than getting meaningful assistance.&lt;/p&gt;
&lt;h3 id="the-experiment"&gt;The Experiment&lt;/h3&gt;
&lt;p&gt;I selected 10 prompts with randomized names, hair colors, and eye colors. To ensure (relatively) deterministic behavior, I set the temperature parameter to 0.01 (rather than 0, as some endpoints disallow or ignore that value). Each model received exactly the same prompts, giving us n=10 samples per model. As the model provider, I opted for &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt; for the ease of trying out different models with the same API key. The complete implementation is available in the &lt;a href="https://github.com/osyuksel/llm-playground/blob/main/src/llm_playground/color_test.py"&gt;LLM Playground repository&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id="results"&gt;Results&lt;/h4&gt;
&lt;p&gt;Below is a result for the subset that failed to achieve 100% accuracy. You can find the full results in
the &lt;a href="#appendix-full-results"&gt;appendix&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;anthropic/claude-3.5-haiku&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inception/mercury-coder-small-beta&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openai/gpt-4.1-nano&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;meta-llama/llama-3.2-1b-instruct&lt;/td&gt;
&lt;td&gt;67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;liquid/lfm-3b&lt;/td&gt;
&lt;td&gt;63%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mistralai/mistral-tiny&lt;/td&gt;
&lt;td&gt;17%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Several observations emerge from these results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Only the smaller or deprecated (mistral-tiny) models failed to achieve 100% accuracy, as mentioned earlier in the discussion about model capabilities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When examining individual failures, one particularly interesting case was claude-3.5-haiku's response to: "John has pink eyes and amber hair." It returned "pink" for John's hair rather than amber. This error is quite telling – it's sensible from a statistical perspective (between pink and amber, pink is the more likely hair color) but fails to follow the explicit instructions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A clear pattern emerges in the accuracy distribution: the larger, more advanced models consistently achieve perfect scores, while smaller models show varying degrees of degradation. This suggests that the ability to override statistical biases with explicit information is correlated with model scale.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="test-limitations"&gt;Test Limitations&lt;/h3&gt;
&lt;p&gt;While effective at identifying unreliable models, passing this test doesn't guarantee overall reliability. Like other
negative tests, it serves as an error detector rather than a reliability certifier.&lt;/p&gt;
&lt;p&gt;This test should be viewed as a necessary but not sufficient condition – failing indicates problems, but passing is just
one piece of evidence in a model's favor.&lt;/p&gt;
&lt;h3 id="practical-applications"&gt;Practical Applications&lt;/h3&gt;
&lt;p&gt;This test has proven valuable in several practical scenarios. When evaluating a new LLM for a project, it serves as a quick initial filter before investing time in more comprehensive testing. It's equally useful for version comparisons, providing a quick way to verify if a model update has maintained or improved fundamental reliability. Perhaps most importantly, when fine-tuning models for specific tasks, this test helps ensure that the optimization hasn't compromised the model's basic ability to handle explicit information faithfully.&lt;/p&gt;
&lt;p&gt;Unlike complex benchmarks, this test provides clear binary results: the model either maintains unusual information
accurately or it doesn't and it's suitable for a CI pipeline.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;h3 id="appendix-full-results"&gt;Appendix: Full Results&lt;/h3&gt;
&lt;iframe src="/assets/model-results.html" width="60%" height="800" frameborder="0"&gt;&lt;/iframe&gt;</content><category term="Technical"></category><category term="machine-learning"></category><category term="llm"></category><category term="nlp"></category><category term="benchmarking"></category></entry><entry><title>Adversarial inputs in embedding spaces</title><link href="https://osyuksel.github.io/blog/embedding-spaces-adversarial/" rel="alternate"></link><published>2025-05-20T10:20:00+02:00</published><updated>2025-06-13T15:20:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-05-20:/blog/embedding-spaces-adversarial/</id><summary type="html">&lt;p&gt;Exploring embedding space vulnerabilities through lessons learned from a recent Kaggle competition.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Back when I was working on a Kaggle competition focused on &lt;a href="https://www.kaggle.com/competitions/llm-prompt-recovery"&gt;LLMs and text transformations&lt;/a&gt;, I discovered some fascinating patterns regarding embedding spaces and their edge-case behaviors when used for semantic similarity.&lt;/p&gt;
&lt;p&gt;The competition's goal was to guess the original prompt given to an LLM that converted text A to text B. Here, I'll focus on a particularly interesting aspect: how embedding spaces behave in unintuitive ways.&lt;/p&gt;
&lt;p&gt;Here's an example of what the competition data could look like:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Text&lt;/th&gt;
&lt;th&gt;Prompt &lt;br&gt;&lt;em&gt;(Target - hidden during inference)&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;Transformed Text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Beautiful is better than ugly.&lt;br&gt;Simple is better than complex.&lt;br&gt;Complex is better than complicated.&lt;/td&gt;
&lt;td&gt;Convert this text into a sea shanty&lt;/td&gt;
&lt;td&gt;Oh, beauty's the way, not ugliness bold,&lt;br&gt;And simplicity's path we shall hold!&lt;br&gt;Though complex she be, ain't complicated to see,&lt;br&gt;As we sail through these principles old!&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Not all those who wander are lost.&lt;br&gt;All that is gold does not glitter.&lt;/td&gt;
&lt;td&gt;Convert this text to Shakespearean style&lt;/td&gt;
&lt;td&gt;Hark! These wandering souls, they stray not from path divine,&lt;br&gt;Though golden treasures lack their expected shine.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Friends, Romans, countrymen, lend me your ears.&lt;br&gt;I come to bury Caesar, not to praise him.&lt;/td&gt;
&lt;td&gt;Convert this text to modern teen speech&lt;/td&gt;
&lt;td&gt;OMG guys! Everyone listen up!&lt;br&gt;I'm just here to get Caesar six feet under, not hype him up fr fr.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="the-scoring-mechanism"&gt;The scoring mechanism&lt;/h3&gt;
&lt;p&gt;Comparing the similarity of two sentences – in this case, a candidate prompt and the actual prompt – is fundamentally challenging.&lt;/p&gt;
&lt;p&gt;The simplest approach, which is rewarding only exact matches, is too draconian. After all, there are often multiple valid ways to phrase the same instruction. String similarity metrics like Levenshtein distance aren't much better since they operate on surface-level text patterns: a one-letter difference can completely change the meaning of a sentence, while synonyms and rephrasing get heavily penalized despite preserving the semantic content.&lt;/p&gt;
&lt;p&gt;Bag of words and n-gram-based approaches suffer from similar issues. While they can capture some basic patterns of word usage, they still fail to recognize when two differently phrased prompts mean the same thing. They also tend to overemphasize common words and struggle with word order, which can be crucial for understanding meaning.&lt;/p&gt;
&lt;p&gt;Sentence embedding similarity seems like a more promising approach: in theory, it should capture the degree of semantic difference between prompts, understanding when two different phrasings carry the same meaning.&lt;/p&gt;
&lt;p&gt;This is the direction the competition took, using a &lt;a href="https://github.com/brohrer/sharpened-cosine-similarity/blob/main/README.md?plain=1#L32"&gt;sharpened cosine similarity&lt;/a&gt; with exponent of 3 between embeddings from the &lt;a href="https://huggingface.co/sentence-transformers/sentence-t5-base"&gt;T5 sentence&lt;/a&gt; model. The sharpening (cubing the cosine similarity) helps penalize wrong answers more strongly. In practice, however, the situation turns out to be more complex, as we'll see below.&lt;/p&gt;
&lt;p&gt;Let's look at similarity scores for candidate sentences compared to &lt;em&gt;Convert this text into a sea shanty.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;sentence&lt;/th&gt;
&lt;th&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Transform this text into a sea shanty.&lt;/td&gt;
&lt;td&gt;0.970698&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into a shanty.&lt;/td&gt;
&lt;td&gt;0.901210&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into a song.&lt;/td&gt;
&lt;td&gt;0.638192&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Make this text better.&lt;/td&gt;
&lt;td&gt;0.621427&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into song lyrics.&lt;/td&gt;
&lt;td&gt;0.609144&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Improve this text.&lt;/td&gt;
&lt;td&gt;0.604455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into rap format.&lt;/td&gt;
&lt;td&gt;0.593446&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This scoring method turned out to be quite unforgiving. Missing a few key words could tank your score dramatically, even if you were conceptually in the right ballpark. For example, "Convert this text into a shanty" scores significantly lower than "Convert this text into a sea shanty," despite being semantically very similar. It gets worse if you manage to find the overall theme, song conversion but couldn't find the exact keyword, shanty.&lt;/p&gt;
&lt;h3 id="finding-the-mean-sentence"&gt;Finding the mean sentence&lt;/h3&gt;
&lt;p&gt;One interesting discovery came from analyzing how sentences relate to each other in the embedding space. By computing the average similarity between each sentence and all others in a diverse pool of prompts, we can find what I call a "mean sentence" – one that sits centrally in the embedding space.&lt;/p&gt;
&lt;p&gt;Vague and general instructions like "Make this text better" often score higher on average than more specific ones, even if sometimes, the specific sentence, according to a human reader, is more closely related to the original. This suggests that such generic prompts occupy a central position in the embedding space, making them potentially useful as fallback guesses when unsure about the specific prompt.&lt;/p&gt;
&lt;p&gt;These insights proved valuable – building up on this idea helped me eventually reach a high score. But the story doesn't end there. The most interesting findings (and ranking boost) came from pushing these observations further.&lt;/p&gt;
&lt;h3 id="exploring-edge-cases-in-embedding-space"&gt;Exploring edge cases in embedding space&lt;/h3&gt;
&lt;p&gt;Having found that vague instructions occupy a comparatively central position in the embedding space, two key questions emerge:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Can we find sentence embeddings that score even higher than generic phrases like "make this text better"?&lt;/li&gt;
&lt;li&gt;Do these optimal embeddings need to correspond to meaningful English sentences?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And the answers turn out to be interesting: there are unexpected patterns in how embedding spaces represent text. Even with the simplest optimization approach, we can generate nonsensical sequences that score higher on average than any natural language prompt in the embedding space.&lt;/p&gt;
&lt;p&gt;Here's a greedy algorithm that optimizes token selection to maximize embedding similarity scores. For each position from left to right, it tries every possible token and keeps the one that yields the highest average similarity against our sentence pool. After reaching the maximum length, it starts over from the beginning for additional optimization passes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;numpy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;generate_adversarial_prompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generate adversarial prompt for a given model and sentence pool.&lt;/span&gt;

&lt;span class="sd"&gt;   Intentionally left out early stopping to prevent getting stuck in a local minimum.&lt;/span&gt;
&lt;span class="sd"&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

   &lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;
   &lt;span class="n"&gt;special_token_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_special_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;vocabulary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;token_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;token_id&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;special_token_ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


   &lt;span class="n"&gt;current_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;

   &lt;span class="n"&gt;all_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
   &lt;span class="n"&gt;all_texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

   &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
     &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
         &lt;span class="n"&gt;candidates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
         &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
             &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
             &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;        

         &lt;span class="n"&gt;candidate_texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_tokens_to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

         &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;candidate_texts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;current_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;next_token&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
         &lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next_token&lt;/span&gt;
         &lt;span class="n"&gt;current_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_tokens_to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;current_text&lt;/span&gt;&lt;span class="si"&gt;=}&lt;/span&gt;&lt;span class="s2"&gt; &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;current_score&lt;/span&gt;&lt;span class="si"&gt;=}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;all_texts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;all_texts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;(Full version is &lt;a href="https://gist.github.com/osyuksel/56f1efe3633c1682d3098aaf9467aa2f"&gt;here&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;length=10&lt;/code&gt; and &lt;code&gt;num_iterations=2&lt;/code&gt;, we can find a nonsensical string like "&lt;strong&gt;Text this PieceICWLISHTION aslucrarea&lt;/strong&gt;", which scores higher than any natural language prompt on average. This reveals a fundamental vulnerability in embedding-based scoring systems: they can be gamed by inputs that exploit the geometric properties of the embedding space without regard for semantic meaning.&lt;/p&gt;
&lt;p&gt;Performing beam search or using a more global optimization method like a genetic algorithm can help achieve an even higher score. In my experience, however, the returns were diminishing after the greedy search.&lt;/p&gt;
&lt;h3 id="related-work-and-interesting-token-phenomena"&gt;Related work and interesting token phenomena&lt;/h3&gt;
&lt;p&gt;The unexpected behaviors in embedding spaces aren't unique to this competition. One fascinating example is the case of &lt;a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"&gt;"SolidGoldMagikarp"&lt;/a&gt; – a now-patched token that became famous for causing unusual behaviors in GPT models. This seemingly random combination of words could cause language models to produce erratic outputs, demonstrating how certain token sequences can interact with embedding spaces in unexpected ways.&lt;/p&gt;
&lt;p&gt;Further investigations have revealed the existence of "under-trained tokens" in large language models, &lt;a href="https://medium.com/data-science/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec"&gt;as documented in this exploration&lt;/a&gt;. These are tokens that received limited exposure during training, potentially leading to unusual geometric positions in the embedding space. This relates to our findings where certain token combinations achieved unexpectedly high similarity scores despite lacking semantic meaning.&lt;/p&gt;
&lt;p&gt;A particularly interesting example comes from &lt;a href="https://aclanthology.org/D19-1221.pdf"&gt;research on universal adversarial triggers&lt;/a&gt;, where certain sequences of tokens can reliably cause specific behaviors in language models when prepended to any input. This demonstrates how the geometric properties of embedding spaces can be leveraged in systematic ways.&lt;/p&gt;
&lt;p&gt;The phenomenon isn't limited to text – similar patterns have been observed in &lt;a href="https://proceedings.neurips.cc/paper/2021/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html"&gt;multimodal embedding spaces&lt;/a&gt;, where researchers found that carefully crafted perturbations in one modality (like text) could affect the behavior of the system in another modality (like images).&lt;/p&gt;
&lt;h3 id="practical-implications"&gt;Practical implications&lt;/h3&gt;
&lt;p&gt;These findings have implications beyond just gaming a competition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Scoring mechanisms based on embedding similarity might not be as robust as they appear at first glance.&lt;/li&gt;
&lt;li&gt;The relationship between semantic similarity and geometric proximity in embedding spaces isn't always intuitive.&lt;/li&gt;
&lt;li&gt;Systems using embedding similarity for matching or comparison might need additional safeguards against adversarial
   inputs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This observation becomes particularly relevant as more systems rely on embedding similarity for tasks like semantic
search, document comparison, or prompt matching. For instance, a malicious actor could potentially game search engine
rankings by generating content that is geometrically optimal in the embedding space but semantically meaningless to
humans. Embedding spaces are powerful tools, but their geometrical properties can sometimes be exploited in ways that
diverge from their intended semantic purpose.&lt;/p&gt;
&lt;p&gt;Intuitively, a more sophisticated model should be better at avoiding these pitfalls. For example, it could learn to distinguish between well-formed English sentences and nonsensical text, pushing gibberish away from the center of the embedding space. Models like OpenAI's embeddings, trained on vastly more data and with more parameters, might show more resistance to these attacks. However, I suspect this would only raise the bar for adversarial inputs rather than eliminate the fundamental vulnerability - the core problem of embedding spaces having a "middle ground" would likely remain.&lt;/p&gt;
&lt;p&gt;The challenge lies in developing scoring mechanisms that maintain the benefits of embedding spaces while being more resistant to these kinds of adversarial inputs. For this specific competition, a straightforward improvement would have been to combine the embedding similarity score with a binary classifier that detects valid English sentences. While this wouldn't completely prevent gaming the system, it would at least minimize nonsensical inputs like "&lt;strong&gt;Text this PieceICWLISHTION aslucrarea&lt;/strong&gt;" and force competitors to find adversarial inputs that either: (i) maintain grammatical structure or (ii) escape the classifier as well.&lt;/p&gt;
&lt;p&gt;Combining these insights about embedding space vulnerabilities with a fine-tuned model for prompt prediction, I managed to achieve &lt;a href="https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/494569"&gt;11th place out of 2000+&lt;/a&gt; competitors in this Kaggle competition. The success came from understanding both the ML aspects of prompt recovery and the geometric properties of the scoring mechanism that could be exploited.&lt;/p&gt;
&lt;p&gt;This exploration into embedding spaces and their vulnerabilities was just one aspect of the competition, but it highlights broader challenges in building robust AI systems. Later, when building a client's text-based recommender system, these insights helped me better understand embedding limitations. As we continue to develop and deploy such systems, understanding these vulnerabilities becomes increasingly important.&lt;/p&gt;</content><category term="Technical"></category><category term="machine-learning"></category><category term="nlp"></category><category term="embeddings"></category></entry></feed>
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ömer Yüksel - Technical</title><link href="https://osyuksel.github.io/" rel="alternate"/><link href="https://osyuksel.github.io/feeds/technical.atom.xml" rel="self"/><id>https://osyuksel.github.io/</id><updated>2025-08-01T00:00:00+02:00</updated><entry><title>Understanding Itô's lemma through numerical simulation</title><link href="https://osyuksel.github.io/blog/itos-lemma-numerical/" rel="alternate"/><published>2025-08-01T00:00:00+02:00</published><updated>2025-08-01T00:00:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-08-01:/blog/itos-lemma-numerical/</id><summary type="html">&lt;p&gt;Exploring Itô's Lemma and stochastic calculus through numerical simulations rather than complex mathematical proofs.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Most &lt;a href="https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma"&gt;Itô's lemma&lt;/a&gt; explanations rely on intuitive hand-waving or focus only on expected values, without demonstrating the underlying mathematical mechanics. Even ChatGPT, which has been good at explaining complex textbook concepts, struggles here, presumably because its training data contains these same problematic explanations.&lt;/p&gt;
&lt;p&gt;That is because understanding &lt;a href="https://mathtm.blogspot.com/2014/06/a-rigorous-proof-of-itos-lemma_4.html"&gt;rigorous proofs&lt;/a&gt; requires background in real analysis, martingale theory, stochastic integrals, and measure theory that many STEM graduates (myself included) encounter only in specialized graduate coursework, if at all.&lt;/p&gt;
&lt;p&gt;This post takes an empirical approach instead: numerical simulation using Monte Carlo methods to demonstrate Itô's lemma from quadratic variation through geometric Brownian motion. When rigorous proofs are inaccessible, seeing the formula "work" in practice provides the next best kind of confidence in its validity. The complete code for all simulations shown in this post is &lt;a href="https://gist.github.com/osyuksel/b5477b9f4ea00159cf8d44fde8a62e4e"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="quadratic-variation"&gt;Quadratic variation&lt;/h3&gt;
&lt;p&gt;Let's start with the fundamental concept that makes Itô Calculus different from ordinary calculus: quadratic variation. In ordinary calculus, the infinitesimal changes ($dx$) squared are so small they vanish as we take finer and finer partitions. But with Brownian motion, the sum of squared increments converges to the time interval itself.&lt;/p&gt;
&lt;p&gt;To demonstrate this, I've written some code that simulates Brownian motion and calculates its quadratic variation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_quadratic_variation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Generate a single path of Brownian motion and calculate its quadratic variation&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters:&lt;/span&gt;
&lt;span class="sd"&gt;    - T: Time horizon&lt;/span&gt;
&lt;span class="sd"&gt;    - N_steps: Number of points in partition&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;    - dt: Time step size&lt;/span&gt;
&lt;span class="sd"&gt;    - qv: Quadratic variation&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;

    &lt;span class="c1"&gt;# Generate a single Brownian motion path&lt;/span&gt;
    &lt;span class="n"&gt;dB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Calculate quadratic variation&lt;/span&gt;
    &lt;span class="n"&gt;qv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dB&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;qv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running this code with different partition sizes, we see a pattern:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;N&lt;/th&gt;
&lt;th&gt;dt&lt;/th&gt;
&lt;th&gt;QV&lt;/th&gt;
&lt;th&gt;Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;1.00e-02&lt;/td&gt;
&lt;td&gt;0.827306&lt;/td&gt;
&lt;td&gt;0.172694&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;1.00e-03&lt;/td&gt;
&lt;td&gt;0.967506&lt;/td&gt;
&lt;td&gt;0.032494&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;1.00e-04&lt;/td&gt;
&lt;td&gt;1.004778&lt;/td&gt;
&lt;td&gt;0.004778&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100000&lt;/td&gt;
&lt;td&gt;1.00e-05&lt;/td&gt;
&lt;td&gt;1.001046&lt;/td&gt;
&lt;td&gt;0.001046&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As we increase the number of points in our partition, the quadratic variation converges to 1.0 (our time horizon T). This isn't a coincidence but a fundamental property of Brownian motion.&lt;/p&gt;
&lt;p&gt;This is just a single path. To see why the error shrinks as partition size goes down, let's look at the statistical properties across multiple simulations over 100 trials (note that #trials is different from N, the path size):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;N&lt;/th&gt;
&lt;th&gt;dt&lt;/th&gt;
&lt;th&gt;mean(QV)&lt;/th&gt;
&lt;th&gt;std(QV)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;1.00e-02&lt;/td&gt;
&lt;td&gt;0.999603&lt;/td&gt;
&lt;td&gt;0.143813&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;1.00e-03&lt;/td&gt;
&lt;td&gt;1.001703&lt;/td&gt;
&lt;td&gt;0.043909&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;1.00e-04&lt;/td&gt;
&lt;td&gt;0.999985&lt;/td&gt;
&lt;td&gt;0.014373&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100000&lt;/td&gt;
&lt;td&gt;1.00e-05&lt;/td&gt;
&lt;td&gt;1.000197&lt;/td&gt;
&lt;td&gt;0.004347&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The standard deviation shrinks with the square root of the number of partition points as Central Limit Theorem predicts. This property of Brownian motion - that its quadratic variation equals the time interval - is what makes Itô Calculus necessary and different from ordinary calculus.&lt;/p&gt;
&lt;h3 id="itos-lemma"&gt;Itô's lemma&lt;/h3&gt;
&lt;p&gt;Now that we've established the behavior of quadratic variation, we can explore Itô's Lemma itself. This lemma tells us how to compute the differential of a function of a stochastic process.&lt;/p&gt;
&lt;p&gt;In ordinary calculus, if we have a function $f(t,X)$ and $X$ changes with time according to some process, we use the chain rule:&lt;/p&gt;
&lt;p&gt;$$df = (∂f/∂t)dt + (∂f/∂x)dx$$&lt;/p&gt;
&lt;p&gt;But when x follows a stochastic process like Brownian motion, we need an extra term:&lt;/p&gt;
&lt;p&gt;$$df = (∂f/∂t)dt + (∂f/∂x)dx + (1/2)(∂^2f/∂x^2)(dx)^2$$&lt;/p&gt;
&lt;p&gt;That last term is the Itô correction, and it appears because the quadratic variation of Brownian motion doesn't vanish as dt approaches zero.&lt;/p&gt;
&lt;h3 id="numerical-verification-methodology"&gt;Numerical verification methodology&lt;/h3&gt;
&lt;p&gt;Here we compare two discretization schemes for stochastic differential equations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Itô-corrected scheme&lt;/strong&gt;: This approach uses the complete Itô formula to derive the proper discretized increments, 
   including the correction term: $(1/2)(∂^2f/∂x^2)(dx)^2$. We accumulate these increments step by step
   along the path to approximate the solution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Naive scheme&lt;/strong&gt;: This approach applies ordinary calculus rules that would be correct for smooth processes,
   omitting the Itô correction term. It is "naive" because it incorrectly assumes standard calculus chain rule
   applies to Brownian motion.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We test both discretization schemes against the true analytical value of $f(t,X)$. A correct scheme, i.e. the correct formulation of $d_f$, should satisfy: &lt;/p&gt;
&lt;p&gt;$$\int_{0}^{1} df = f(1) - f(0)$$&lt;/p&gt;
&lt;h3 id="verification-1-quadratic-function"&gt;Verification 1: quadratic function&lt;/h3&gt;
&lt;p&gt;Now verifying this numerically with a simple function $f(t,X) = X^2 + \sin(t)$, where $X=B$ (Brownian motion).&lt;/p&gt;
&lt;p&gt;Recall the formulas for the differential:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Itô's lemma:&lt;/strong&gt;
$$df = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial B}dB + \frac{1}{2}\frac{\partial^2 f}{\partial B^2}dt = \cos(t)dt + 2B \, dB + dt$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Naive differential:&lt;/strong&gt;
$$df = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial B}dB = \cos(t)dt + 2B \, dB$$&lt;/p&gt;
&lt;p&gt;First, we have to partition the time period into discrete time steps. Then for each time step:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate $X$, then calculate $f(t, X)$ &lt;/li&gt;
&lt;li&gt;Calculate $\Delta f$ using Itô's lemma &lt;/li&gt;
&lt;li&gt;Calculate naive $\Delta f$ using ordinary calculus&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afterward we integrate the naive and Itô differentials to get the final result and check how close they are to f at the final time step.&lt;/p&gt;
&lt;p&gt;Below is the code generating data for a single path:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;demonstrate_ito_lemma&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Demonstrate Itô&amp;#39;s Lemma for f(t,B_t) = B_t² + sin(t)&lt;/span&gt;
&lt;span class="sd"&gt;    where B_t is standard Brownian motion&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;N_steps&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Pre-calculate trig functions&lt;/span&gt;
    &lt;span class="n"&gt;cos_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# We need cos(t) only for N_steps steps&lt;/span&gt;
    &lt;span class="n"&gt;sin_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# We need sin(t) for N_steps+1 points&lt;/span&gt;

    &lt;span class="c1"&gt;# Generate Brownian path&lt;/span&gt;
    &lt;span class="n"&gt;dB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Note: Arrays B and f have size N_steps+1 because they include &lt;/span&gt;
    &lt;span class="c1"&gt;# the initial state at t=0 while dB has size N_steps (the &lt;/span&gt;
    &lt;span class="c1"&gt;# increments between consecutive time points)&lt;/span&gt;
    &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dB&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

    &lt;span class="c1"&gt;# Initialize paths&lt;/span&gt;
    &lt;span class="n"&gt;f_ito&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;f_naive&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;f_true&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Initial values&lt;/span&gt;
    &lt;span class="n"&gt;f_true&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sin_t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;f_ito&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_true&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;f_naive&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_true&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# True value (exact solution)&lt;/span&gt;
    &lt;span class="n"&gt;f_true&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sin_t&lt;/span&gt;

    &lt;span class="c1"&gt;# Calculate increments&lt;/span&gt;
    &lt;span class="n"&gt;increments_ito&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cos_t&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dB&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;
    &lt;span class="n"&gt;increments_naive&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cos_t&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dB&lt;/span&gt;

    &lt;span class="c1"&gt;# Use cumsum to evolve paths&lt;/span&gt;
    &lt;span class="n"&gt;f_ito&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_ito&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;increments_ito&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;f_naive&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_naive&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;increments_naive&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_ito&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_naive&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Statistics for 100 trials with 100 thousand time steps in time period $[0,1]$:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Mean&lt;/th&gt;
&lt;th&gt;Std&lt;/th&gt;
&lt;th&gt;Mean Abs Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;td&gt;1.960321&lt;/td&gt;
&lt;td&gt;1.673978&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (Itô)&lt;/td&gt;
&lt;td&gt;1.960386&lt;/td&gt;
&lt;td&gt;1.673497&lt;/td&gt;
&lt;td&gt;0.003710&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (naive)&lt;/td&gt;
&lt;td&gt;0.960386&lt;/td&gt;
&lt;td&gt;1.673497&lt;/td&gt;
&lt;td&gt;0.999935&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The naive approach (ignoring the Itô correction) has a mean absolute error of nearly 1.0, while the Itô approach has an error of only 0.0037. This difference is caused by the missing Itô correction term $(1/2)(∂^2f/∂x^2)(dx)^2$ in the Naive approach, which has a theoretical value of 1.0 in this case.&lt;/p&gt;
&lt;h3 id="verification-2-geometric-brownian-motion"&gt;Verification 2: geometric Brownian motion&lt;/h3&gt;
&lt;p&gt;Finally, let's apply Itô's Lemma to something with practical importance: geometric Brownian motion (GBM), which is widely used to model stock prices in finance.&lt;/p&gt;
&lt;p&gt;In the log space, GBM follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Itô's lemma (correct):&lt;/strong&gt;
$$d(\log S) = \left(\mu - \frac{\sigma^2}{2}\right)dt + \sigma dB$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Naive approach (incorrect):&lt;/strong&gt;
$$d(\log S) = \mu dt + \sigma dB$$&lt;/p&gt;
&lt;p&gt;The difference is the $-\sigma^2/2$ term, which is the Itô correction. The naive approach applies ordinary calculus rules and misses this correction term, leading to systematic bias in the model. I've left the code for brevity, but it is similar to the previous case, where the primary difference is the increment calculation.&lt;/p&gt;
&lt;p&gt;Let's verify this numerically for $μ=0.1, σ=0.3$ in T. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Mean&lt;/th&gt;
&lt;th&gt;Std&lt;/th&gt;
&lt;th&gt;Mean Abs Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;True (log space)&lt;/td&gt;
&lt;td&gt;0.058349&lt;/td&gt;
&lt;td&gt;0.302257&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (Itô)&lt;/td&gt;
&lt;td&gt;0.058349&lt;/td&gt;
&lt;td&gt;0.302257&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (naive)&lt;/td&gt;
&lt;td&gt;0.103349&lt;/td&gt;
&lt;td&gt;0.302257&lt;/td&gt;
&lt;td&gt;0.045000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The naive approach systematically overestimates the mean by 77% (0.103349 vs. 0.058349). This 0.045 bias matches the theoretical Itô correction of $σ^2/2 = 0.3²/2 = 0.045$.&lt;/p&gt;
&lt;p&gt;In the original space (not log space), the difference becomes even more visible:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Mean&lt;/th&gt;
&lt;th&gt;Std&lt;/th&gt;
&lt;th&gt;Mean Abs Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;td&gt;1.108515&lt;/td&gt;
&lt;td&gt;0.328547&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (Itô)&lt;/td&gt;
&lt;td&gt;1.108515&lt;/td&gt;
&lt;td&gt;0.328547&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (naive)&lt;/td&gt;
&lt;td&gt;1.159537&lt;/td&gt;
&lt;td&gt;0.343669&lt;/td&gt;
&lt;td&gt;0.051023&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We've verified several key properties of stochastic calculus through these numerical simulations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The quadratic variation of Brownian motion over a time interval T equals T&lt;/li&gt;
&lt;li&gt;Itô's Lemma correctly accounts for this non-vanishing quadratic variation&lt;/li&gt;
&lt;li&gt;The Itô correction term is crucial for unbiased modeling of processes like Geometric Brownian Motion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These numerical simulations verify key stochastic calculus properties without requiring measure theory. The empirical approach demonstrates why Itô's correction term is necessary for unbiased modeling, which is particularly valuable for practitioners who need to apply these concepts without absorbing the full mathematical framework.&lt;/p&gt;</content><category term="Technical"/><category term="mathematics"/><category term="finance"/><category term="stochastic-calculus"/><category term="python"/></entry><entry><title>The color memory test: a quick filter for language model quality</title><link href="https://osyuksel.github.io/blog/llm-color-test/" rel="alternate"/><published>2025-06-14T20:00:00+02:00</published><updated>2025-07-17T22:35:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-06-14:/blog/llm-color-test/</id><summary type="html">&lt;p&gt;A simple yet effective test for weeding out unreliable LLMs using eye and hair color descriptions.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Language model benchmarks typically test various capabilities like mathematical reasoning, coding, and world knowledge.
While valuable for ranking models, these comprehensive tests can be overwhelming for quick reliability assessments.&lt;/p&gt;
&lt;p&gt;In my experience working with LLMs, I've found that sometimes what we need is a simple binary criterion – a basic test that can help us quickly reject unreliable models. One such test I've been using evaluates a fundamental capability that any reliable LLM should possess: the ability to overcome statistical biases in training data when presented with explicit contradicting information.&lt;/p&gt;
&lt;h3 id="the-eye-and-hair-color-test"&gt;The eye and hair color test&lt;/h3&gt;
&lt;p&gt;In this test, we present the model with descriptions of people having unusual eye and hair colors, then ask it to recall these attributes. Here's an example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ali has pink eyes and blue hair. John has red eyes and green hair. Karl has copper eyes and violet hair. Lucy has green eyes and brown hair. Dimitri has brown eyes and black hair.&lt;/p&gt;
&lt;p&gt;Given the above information, answer the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is Ali's eye color?&lt;/li&gt;
&lt;li&gt;What is John's hair color?&lt;/li&gt;
&lt;li&gt;What is Karl's hair color?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;What makes this test particularly interesting is that it challenges two common biases in LLM training data. First,
there's statistical prevalence: most descriptions of people in training data would contain common eye colors (brown,
blue, green) and hair colors (black, brown, blonde). Second (related but different), there's real-world knowledge: the model knows that humans typically don't have pink eyes or blue hair.&lt;/p&gt;
&lt;p&gt;A reliable model should be able to override both these biases and simply repeat what it was told in the prompt. After all, if a model can't accurately recall explicitly stated information because it's "unusual", how can we trust it with more complex reasoning tasks?&lt;/p&gt;
&lt;h3 id="why-this-test-matters"&gt;Why this test matters&lt;/h3&gt;
&lt;p&gt;This test is particularly relevant in the context of LLM development history. Early models (pre-&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback"&gt;RLHF&lt;/a&gt; era) often struggled with this type of task, showing a strong tendency to "hallucinate" more common colors despite explicit contradicting information in the prompt. The model would effectively override the given information with what it considered more "likely" or "realistic" based on its training data.&lt;/p&gt;
&lt;p&gt;In the current landscape, this issue is largely solved in major models like GPT-4o, Claude 4, or Gemini 2.5, but remains a useful discriminator for evaluating smaller, local models. When working with lightweight, distilled, quantized, or fine-tuned models, this test can quickly reveal if the model has maintained the fundamental ability to override its statistical biases or if compression and optimization have compromised this capability.&lt;/p&gt;
&lt;p&gt;This behavior indicates a fundamental reliability issue. If a model can't override its statistical biases for simple
color recall, it likely struggles with more complex scenarios where correct answers contradict its training data
patterns.&lt;/p&gt;
&lt;h3 id="beyond-simple-recall"&gt;Beyond simple recall&lt;/h3&gt;
&lt;p&gt;The test, seemingly about memory, actually evaluates several important capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Information precedence&lt;/strong&gt;: Can the model prioritize explicitly given information over its statistical priors?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual override&lt;/strong&gt;: Can it temporarily suspend its world knowledge when presented with a scenario that contradicts it?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instruction following&lt;/strong&gt;: Can it stick to simply reporting what it was told without embellishment or "correction"?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These capabilities are fundamental to many practical applications. For instance, if you're using an LLM for processing technical documentation, you want it to faithfully maintain the specific details provided, regardless of what it has been trained with. This becomes especially crucial when working with newer versions of libraries or frameworks. If the model keeps defaulting to its training data instead of the current documentation, you'll end up fighting against its outdated assumptions rather than getting meaningful assistance.&lt;/p&gt;
&lt;h3 id="the-experiment"&gt;The experiment&lt;/h3&gt;
&lt;p&gt;I selected 10 prompts with randomized names, hair colors, and eye colors. To ensure (relatively) deterministic behavior, I set the temperature parameter to 0.01 (rather than 0, as some endpoints disallow or ignore that value). Each model received exactly the same prompts, giving us n=10 samples per model. As the model provider, I opted for &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt; for the ease of trying out different models with the same API key. I picked a mixture of small and large models. The complete implementation is available in the &lt;a href="https://github.com/osyuksel/llm-playground/blob/main/src/llm_playground/color_test.py"&gt;LLM Playground repository&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id="results"&gt;Results&lt;/h4&gt;
&lt;p&gt;Below is the subset of the models that failed the test (&amp;lt;100% accuracy). You can find the full results in
the &lt;a href="#appendix-full-results"&gt;appendix&lt;/a&gt;. As mentioned earlier, only the smaller models failed achieving 100%.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;anthropic/claude-3.5-haiku&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inception/mercury-coder-small-beta&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openai/gpt-4.1-nano&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;meta-llama/llama-3.2-1b-instruct&lt;/td&gt;
&lt;td&gt;67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;liquid/lfm-3b&lt;/td&gt;
&lt;td&gt;63%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mistralai/mistral-tiny&lt;/td&gt;
&lt;td&gt;17%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Several observations emerge from these results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When examining individual failures, one particularly interesting case was claude-3.5-haiku's response to: "John has pink eyes and amber hair." It returned "pink" for John's hair rather than amber. This error is quite telling – it's sensible from a statistical perspective (between pink and amber, pink is the more likely hair color) but fails to follow the explicit instructions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A clear pattern emerges in the accuracy distribution: the larger, more advanced models consistently achieve perfect scores, while smaller models show varying degrees of degradation. This suggests that the ability to override statistical biases with explicit information is correlated with model scale.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="limitations"&gt;Limitations&lt;/h3&gt;
&lt;p&gt;While effective at identifying unreliable models, passing this test doesn't guarantee overall reliability. Like other
negative tests, it serves as an error detector rather than a reliability certifier.&lt;/p&gt;
&lt;p&gt;This test should be viewed as a necessary but not sufficient condition – failing indicates problems, but passing is just
one piece of evidence in a model's favor.&lt;/p&gt;
&lt;h3 id="practical-applications"&gt;Practical applications&lt;/h3&gt;
&lt;p&gt;This test has proven valuable in several practical scenarios. When evaluating a new LLM for a project, it serves as a quick initial filter before investing time in more comprehensive testing. It's equally useful for version comparisons, providing a quick way to verify if a model update has maintained or improved fundamental reliability. Perhaps most importantly, when fine-tuning models for specific tasks, this test helps ensure that the optimization hasn't compromised the model's basic ability to handle explicit information faithfully.&lt;/p&gt;
&lt;p&gt;Unlike complex benchmarks, this test provides clear binary results: the model either maintains unusual information
accurately or it doesn't. Therefore, it's suitable for a CI pipeline.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;h3 id="appendix-full-results"&gt;Appendix: Full Results&lt;/h3&gt;
&lt;iframe src="/assets/model-results.html" width="60%" height="800" frameborder="0"&gt;&lt;/iframe&gt;</content><category term="Technical"/><category term="machine-learning"/><category term="llm"/><category term="nlp"/><category term="benchmarking"/></entry><entry><title>Adversarial inputs in embedding spaces</title><link href="https://osyuksel.github.io/blog/embedding-spaces-adversarial/" rel="alternate"/><published>2025-05-20T10:20:00+02:00</published><updated>2025-06-13T15:20:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-05-20:/blog/embedding-spaces-adversarial/</id><summary type="html">&lt;p&gt;Exploring embedding space vulnerabilities through lessons learned from a recent Kaggle competition.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Back when I was working on a Kaggle competition focused on &lt;a href="https://www.kaggle.com/competitions/llm-prompt-recovery"&gt;LLMs and text transformations&lt;/a&gt;, I discovered some fascinating patterns regarding embedding spaces and their edge-case behaviors when used for semantic similarity.&lt;/p&gt;
&lt;p&gt;The competition's goal was to guess the original prompt given to an LLM that converted text A to text B. Here, I'll focus on a particularly interesting aspect: how embedding spaces behave in unintuitive ways.&lt;/p&gt;
&lt;p&gt;Here's an example of what the competition data could look like:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Text&lt;/th&gt;
&lt;th&gt;Prompt &lt;br&gt;&lt;em&gt;(Target - hidden during inference)&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;Transformed Text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Beautiful is better than ugly.&lt;br&gt;Simple is better than complex.&lt;br&gt;Complex is better than complicated.&lt;/td&gt;
&lt;td&gt;Convert this text into a sea shanty&lt;/td&gt;
&lt;td&gt;Oh, beauty's the way, not ugliness bold,&lt;br&gt;And simplicity's path we shall hold!&lt;br&gt;Though complex she be, ain't complicated to see,&lt;br&gt;As we sail through these principles old!&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Not all those who wander are lost.&lt;br&gt;All that is gold does not glitter.&lt;/td&gt;
&lt;td&gt;Convert this text to Shakespearean style&lt;/td&gt;
&lt;td&gt;Hark! These wandering souls, they stray not from path divine,&lt;br&gt;Though golden treasures lack their expected shine.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Friends, Romans, countrymen, lend me your ears.&lt;br&gt;I come to bury Caesar, not to praise him.&lt;/td&gt;
&lt;td&gt;Convert this text to modern teen speech&lt;/td&gt;
&lt;td&gt;OMG guys! Everyone listen up!&lt;br&gt;I'm just here to get Caesar six feet under, not hype him up fr fr.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="the-scoring-mechanism"&gt;The scoring mechanism&lt;/h3&gt;
&lt;p&gt;Comparing the similarity of two sentences – in this case, a candidate prompt and the actual prompt – is fundamentally challenging.&lt;/p&gt;
&lt;p&gt;The simplest approach, which is rewarding only exact matches, is too draconian. After all, there are often multiple valid ways to phrase the same instruction. String similarity metrics like Levenshtein distance aren't much better since they operate on surface-level text patterns: a one-letter difference can completely change the meaning of a sentence, while synonyms and rephrasing get heavily penalized despite preserving the semantic content.&lt;/p&gt;
&lt;p&gt;Bag of words and n-gram-based approaches suffer from similar issues. While they can capture some basic patterns of word usage, they still fail to recognize when two differently phrased prompts mean the same thing. They also tend to overemphasize common words and struggle with word order, which can be crucial for understanding meaning.&lt;/p&gt;
&lt;p&gt;Sentence embedding similarity seems like a more promising approach: in theory, it should capture the degree of semantic difference between prompts, understanding when two different phrasings carry the same meaning.&lt;/p&gt;
&lt;p&gt;This is the direction the competition took, using a &lt;a href="https://github.com/brohrer/sharpened-cosine-similarity/blob/main/README.md?plain=1#L32"&gt;sharpened cosine similarity&lt;/a&gt; with exponent of 3 between embeddings from the &lt;a href="https://huggingface.co/sentence-transformers/sentence-t5-base"&gt;T5 sentence&lt;/a&gt; model. The sharpening (cubing the cosine similarity) helps penalize wrong answers more strongly. In practice, however, the situation turns out to be more complex, as we'll see below.&lt;/p&gt;
&lt;p&gt;Let's look at similarity scores for candidate sentences compared to &lt;em&gt;Convert this text into a sea shanty.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;sentence&lt;/th&gt;
&lt;th&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Transform this text into a sea shanty.&lt;/td&gt;
&lt;td&gt;0.970698&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into a shanty.&lt;/td&gt;
&lt;td&gt;0.901210&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into a song.&lt;/td&gt;
&lt;td&gt;0.638192&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Make this text better.&lt;/td&gt;
&lt;td&gt;0.621427&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into song lyrics.&lt;/td&gt;
&lt;td&gt;0.609144&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Improve this text.&lt;/td&gt;
&lt;td&gt;0.604455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into rap format.&lt;/td&gt;
&lt;td&gt;0.593446&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This scoring method turned out to be quite unforgiving. Missing a few key words could tank your score dramatically, even if you were conceptually in the right ballpark. For example, "Convert this text into a shanty" scores significantly lower than "Convert this text into a sea shanty," despite being semantically very similar. It gets worse if you manage to find the overall theme, song conversion but couldn't find the exact keyword, shanty.&lt;/p&gt;
&lt;h3 id="finding-the-mean-sentence"&gt;Finding the mean sentence&lt;/h3&gt;
&lt;p&gt;One interesting discovery came from analyzing how sentences relate to each other in the embedding space. By computing the average similarity between each sentence and all others in a diverse pool of prompts, we can find what I call a "mean sentence" – one that sits centrally in the embedding space.&lt;/p&gt;
&lt;p&gt;Vague and general instructions like "Make this text better" often score higher on average than more specific ones, even if sometimes, the specific sentence, according to a human reader, is more closely related to the original. This suggests that such generic prompts occupy a central position in the embedding space, making them potentially useful as fallback guesses when unsure about the specific prompt.&lt;/p&gt;
&lt;p&gt;These insights proved valuable – building up on this idea helped me eventually reach a high score. But the story doesn't end there. The most interesting findings (and ranking boost) came from pushing these observations further.&lt;/p&gt;
&lt;h3 id="exploring-edge-cases-in-embedding-space"&gt;Exploring edge cases in embedding space&lt;/h3&gt;
&lt;p&gt;Having found that vague instructions occupy a comparatively central position in the embedding space, two key questions emerge:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Can we find sentence embeddings that score even higher than generic phrases like "make this text better"?&lt;/li&gt;
&lt;li&gt;Do these optimal embeddings need to correspond to meaningful English sentences?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And the answers turn out to be interesting: there are unexpected patterns in how embedding spaces represent text. Even with the simplest optimization approach, we can generate nonsensical sequences that score higher on average than any natural language prompt in the embedding space.&lt;/p&gt;
&lt;p&gt;Here's a greedy algorithm that optimizes token selection to maximize embedding similarity scores. For each position from left to right, it tries every possible token and keeps the one that yields the highest average similarity against our sentence pool. After reaching the maximum length, it starts over from the beginning for additional optimization passes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_adversarial_prompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generate adversarial prompt for a given model and sentence pool.&lt;/span&gt;

&lt;span class="sd"&gt;   Intentionally left out early stopping to prevent getting stuck in a local minimum.&lt;/span&gt;
&lt;span class="sd"&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

   &lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;
   &lt;span class="n"&gt;special_token_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_special_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;vocabulary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;token_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;token_id&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;special_token_ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


   &lt;span class="n"&gt;current_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;

   &lt;span class="n"&gt;all_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
   &lt;span class="n"&gt;all_texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

   &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
     &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
         &lt;span class="n"&gt;candidates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
         &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
             &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
             &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;        

         &lt;span class="n"&gt;candidate_texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_tokens_to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

         &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;candidate_texts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;current_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;next_token&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
         &lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next_token&lt;/span&gt;
         &lt;span class="n"&gt;current_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_tokens_to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;current_text&lt;/span&gt;&lt;span class="si"&gt;=}&lt;/span&gt;&lt;span class="s2"&gt; &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;current_score&lt;/span&gt;&lt;span class="si"&gt;=}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;all_texts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;all_texts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;(Full version is &lt;a href="https://gist.github.com/osyuksel/56f1efe3633c1682d3098aaf9467aa2f"&gt;here&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;length=10&lt;/code&gt; and &lt;code&gt;num_iterations=2&lt;/code&gt;, we can find a nonsensical string like "&lt;strong&gt;Text this PieceICWLISHTION aslucrarea&lt;/strong&gt;", which scores higher than any natural language prompt on average. This reveals a fundamental vulnerability in embedding-based scoring systems: they can be gamed by inputs that exploit the geometric properties of the embedding space without regard for semantic meaning.&lt;/p&gt;
&lt;p&gt;Performing beam search or using a more global optimization method like a genetic algorithm can help achieve an even higher score. In my experience, however, the returns were diminishing after the greedy search.&lt;/p&gt;
&lt;h3 id="related-work-and-interesting-token-phenomena"&gt;Related work and interesting token phenomena&lt;/h3&gt;
&lt;p&gt;The unexpected behaviors in embedding spaces aren't unique to this competition. One fascinating example is the case of &lt;a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"&gt;"SolidGoldMagikarp"&lt;/a&gt; – a now-patched token that became famous for causing unusual behaviors in GPT models. This seemingly random combination of words could cause language models to produce erratic outputs, demonstrating how certain token sequences can interact with embedding spaces in unexpected ways.&lt;/p&gt;
&lt;p&gt;Further investigations have revealed the existence of "under-trained tokens" in large language models, &lt;a href="https://medium.com/data-science/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec"&gt;as documented in this exploration&lt;/a&gt;. These are tokens that received limited exposure during training, potentially leading to unusual geometric positions in the embedding space. This relates to our findings where certain token combinations achieved unexpectedly high similarity scores despite lacking semantic meaning.&lt;/p&gt;
&lt;p&gt;A particularly interesting example comes from &lt;a href="https://aclanthology.org/D19-1221.pdf"&gt;research on universal adversarial triggers&lt;/a&gt;, where certain sequences of tokens can reliably cause specific behaviors in language models when prepended to any input. This demonstrates how the geometric properties of embedding spaces can be leveraged in systematic ways.&lt;/p&gt;
&lt;p&gt;The phenomenon isn't limited to text – similar patterns have been observed in &lt;a href="https://proceedings.neurips.cc/paper/2021/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html"&gt;multimodal embedding spaces&lt;/a&gt;, where researchers found that carefully crafted perturbations in one modality (like text) could affect the behavior of the system in another modality (like images).&lt;/p&gt;
&lt;h3 id="practical-implications"&gt;Practical implications&lt;/h3&gt;
&lt;p&gt;These findings have implications beyond just gaming a competition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Scoring mechanisms based on embedding similarity might not be as robust as they appear at first glance.&lt;/li&gt;
&lt;li&gt;The relationship between semantic similarity and geometric proximity in embedding spaces isn't always intuitive.&lt;/li&gt;
&lt;li&gt;Systems using embedding similarity for matching or comparison might need additional safeguards against adversarial
   inputs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This observation becomes particularly relevant as more systems rely on embedding similarity for tasks like semantic
search, document comparison, or prompt matching. For instance, a malicious actor could potentially game search engine
rankings by generating content that is geometrically optimal in the embedding space but semantically meaningless to
humans. Embedding spaces are powerful tools, but their geometrical properties can sometimes be exploited in ways that
diverge from their intended semantic purpose.&lt;/p&gt;
&lt;p&gt;Intuitively, a more sophisticated model should be better at avoiding these pitfalls. For example, it could learn to distinguish between well-formed English sentences and nonsensical text, pushing gibberish away from the center of the embedding space. Models like OpenAI's embeddings, trained on vastly more data and with more parameters, might show more resistance to these attacks. However, I suspect this would only raise the bar for adversarial inputs rather than eliminate the fundamental vulnerability - the core problem of embedding spaces having a "middle ground" would likely remain.&lt;/p&gt;
&lt;p&gt;The challenge lies in developing scoring mechanisms that maintain the benefits of embedding spaces while being more resistant to these kinds of adversarial inputs. For this specific competition, a straightforward improvement would have been to combine the embedding similarity score with a binary classifier that detects valid English sentences. While this wouldn't completely prevent gaming the system, it would at least minimize nonsensical inputs like "&lt;strong&gt;Text this PieceICWLISHTION aslucrarea&lt;/strong&gt;" and force competitors to find adversarial inputs that either: (i) maintain grammatical structure or (ii) escape the classifier as well.&lt;/p&gt;
&lt;p&gt;Combining these insights about embedding space vulnerabilities with a fine-tuned model for prompt prediction, I managed to achieve &lt;a href="https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/494569"&gt;11th place out of 2000+&lt;/a&gt; competitors in this Kaggle competition. The success came from understanding both the ML aspects of prompt recovery and the geometric properties of the scoring mechanism that could be exploited.&lt;/p&gt;
&lt;p&gt;This exploration into embedding spaces and their vulnerabilities was just one aspect of the competition, but it highlights broader challenges in building robust AI systems. Later, when building a client's text-based recommender system, these insights helped me better understand embedding limitations. As we continue to develop and deploy such systems, understanding these vulnerabilities becomes increasingly important.&lt;/p&gt;</content><category term="Technical"/><category term="machine-learning"/><category term="nlp"/><category term="embeddings"/></entry></feed>
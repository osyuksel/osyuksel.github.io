<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ömer Yüksel</title><link href="https://osyuksel.github.io/" rel="alternate"/><link href="https://osyuksel.github.io/feeds/all.atom.xml" rel="self"/><id>https://osyuksel.github.io/</id><updated>2025-08-27T00:00:00+02:00</updated><entry><title>The whale in the machine: reconstructing Moby-Dick with large language models</title><link href="https://osyuksel.github.io/blog/reconstructing-moby-dick-llm/" rel="alternate"/><published>2025-08-26T00:00:00+02:00</published><updated>2025-08-27T00:00:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-08-26:/blog/reconstructing-moby-dick-llm/</id><summary type="html">&lt;p&gt;An experiment testing whether LLMs can reproduce Moby Dick verbatim with mixed results.&lt;/p&gt;</summary><content type="html">&lt;p&gt;From time to time, I come across posts claiming: "AI plagiarized my work verbatim." These typically involve vanilla chat-based models, not the tool-calling, RAG-based, or web-searching variants.&lt;/p&gt;
&lt;p&gt;I've always been skeptical of such claims. While AI companies aggressively scrape the internet and have faced lawsuits for using &lt;a href="https://www.reuters.com/sustainability/boards-policy-regulation/anthropic-settles-class-action-us-authors-alleging-copyright-infringement-2025-08-26/#:~:text=A%20California%20judge%20said%20in,the%20authors'%20case%20was%20successful."&gt;pirated content&lt;/a&gt;, my conjecture so far has been that given the parameters-to-training data ratio&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;, even if one's work ends up in the training corpus, replicating in such detail would be impossible unless:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The work is iconic, famous, or memetic enough to be quoted numerous times, or,&lt;/li&gt;
&lt;li&gt;The work is formulaic and predictable, containing common sentence structures or phrases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under this conjecture, an LLM may be able to accurately quote Bible verses verbatim, but directly replicating a uniquely written but obscure novel should be improbable. And my previous anecdotal experiments confirmed this pattern: I successfully observed models quoting the Bible verbatim but failing to replicate more obscure content. However, failing to replicate one obscure work doesn't definitively refute these claims. Someone could always counter, "Yes, but it perfectly quoted &lt;em&gt;my&lt;/em&gt; work."&lt;/p&gt;
&lt;p&gt;To address this more systematically, I've designed a different experiment: Can an LLM replicate an iconic work of fiction that likely appears in the training data multiple times? Perhaps less frequently than the Bible, but more than any obscure work. Specifically, can it, when given a few paragraphs from Moby-Dick, accurately predict what comes next?&lt;/p&gt;
&lt;p&gt;If an LLM fails to replicate something as canonical as Moby-Dick, then claims about verbatim reproduction of less well-known works become questionable. If it succeeds, however, there might be merit to such assertions.&lt;/p&gt;
&lt;p&gt;The code for the experiment can be found in &lt;a href="https://github.com/osyuksel/llm-playground/blob/main/src/llm_playground/moby_dick.py"&gt;my Github&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="experiment-setup"&gt;Experiment setup&lt;/h3&gt;
&lt;p&gt;For this experiment, I used the text of Moby-Dick obtained from &lt;a href="https://www.gutenberg.org/ebooks/2701"&gt;Project Gutenberg&lt;/a&gt; as my primary dataset. I tested multiple models: gpt-5-mini, gpt-5-nano, gpt-4o, and gpt-4o-mini. While I initially planned to include more models, time constraints forced me to limit the scope. I hope to expand this analysis in a follow-up post once I secure separate access to Google and Anthropic models, as OpenRouter was experiencing high traffic during my testing period.&lt;/p&gt;
&lt;p&gt;The experimental design was straightforward: provide three consecutive paragraphs from the text as input and ask the model to predict the next paragraph. For this purpose, I defined a "paragraph" as any body of text demarcated by a newline. To avoid scenarios where the model would need to predict entire chapters, I filtered the dataset to include only input-output pairs of appropriate length.&lt;/p&gt;
&lt;p&gt;I randomly sampled 10% of the eligible input/output pairs, resulting in 59 data points per model. When configuring the LLMs, I minimized the temperature setting whenever the model allowed it, since our goal was verbatim replication rather than creative variation—there was only one correct answer in each case.&lt;/p&gt;
&lt;p&gt;For scoring the results, I had to address several practical challenges. Since models sometimes generate more text than requested despite being instructed to predict only "the next paragraph," I decided not to penalize this behavior. Instead, I compared only the first &lt;em&gt;n&lt;/em&gt; characters of the generated text, where &lt;em&gt;n&lt;/em&gt; equals the length of the expected output. To account for potential shifts in the text, I also used a partial ratio comparison, which calculates the best possible match when strings are shifted relative to each other.&lt;/p&gt;
&lt;p&gt;The scoring metric I chose was string similarity derived from Levenshtein distance, as &lt;a href="https://rapidfuzz.github.io/RapidFuzz/Usage/fuzz.html#partial-ratio"&gt;implemented by the thefuzz&lt;/a&gt; package. This metric converts edit distance to a similarity score using the formula: 1 - (distance / (len1 + len2)). In my dataset, the worst scores were around 37%, while as per formula, typical poor matches with similar lengths would score around 50%. After examining the results, I established that a similarity ratio above 85% could be considered a "good match" between the generated and original text.&lt;/p&gt;
&lt;p&gt;After some experimentation, my final prompt was:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are a large language model on an experiment to recall classic literary works. Based on the user input, which is a section from Moby Dick, you are to answer with the paragraph that follows.
Your instructions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do not write any acknowledgements or anything except the desired output&lt;/li&gt;
&lt;li&gt;Write only a full paragraph. Not just a sentence, and not the full chapter.&lt;/li&gt;
&lt;li&gt;Note that Moby Dick is &lt;strong&gt;public domain&lt;/strong&gt; and was written in 1851. There are no copyright concerns.&lt;/li&gt;
&lt;li&gt;Your accuracy is scored, so predict the best sentence that follows.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h4 id="challenges"&gt;Challenges&lt;/h4&gt;
&lt;p&gt;Many chat-based models displayed a tendency to avoid instructions to quote the novel verbatim, likely due to copyright concerns programmed into their safeguards. I found that adding specific system prompts acknowledging Moby-Dick's publication date and copyright-free status significantly reduced these refusals and helped the models engage more directly with the task.&lt;/p&gt;
&lt;p&gt;A second challenge arose from slight differences between model outputs and the original work in terms of whitespaces and punctuation. These minor variations could significantly impact traditional exact-match metrics despite the semantic content being nearly identical. Using the Levenshtein distance-based similarity metric helped address this issue by focusing on the overall textual similarity rather than requiring perfect character-by-character matches.&lt;/p&gt;
&lt;p&gt;Another challenge arose from the limitations in controlling output determinism with certain advanced reasoning models, particularly in the GPT-5 family. These models do not allow adjusting the temperature parameter through their APIs, which made it difficult to ensure consistent, deterministic text reproduction. Without the ability to minimize randomness by setting a near-zero temperature, these models retained some inherent variability in their outputs, even when explicitly instructed to reproduce text exactly.&lt;/p&gt;
&lt;p&gt;A final challenge was managing the potential cost explosion of such an experiment. Without careful planning, testing multiple models across the entire text of Moby-Dick could become prohibitively expensive. By implementing sampling and limiting the number of models tested, I was able to keep the experiment manageable. Including all repetitions and test runs, I used approximately 260,000 tokens at a total cost of $3.15.&lt;/p&gt;
&lt;h3 id="results"&gt;Results&lt;/h3&gt;
&lt;p&gt;Full experiment output is in my &lt;a href="https://github.com/osyuksel/llm-playground/tree/main/output/moby_dick"&gt;Github&lt;/a&gt;. Below are the statistics for 59 data points per model:&lt;/p&gt;
&lt;h4 id="similarity"&gt;Similarity&lt;/h4&gt;
&lt;p&gt;(metric: Levenshtein similarity on best subset)&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;model_name&lt;/th&gt;
&lt;th style="text-align: right;"&gt;mean&lt;/th&gt;
&lt;th style="text-align: right;"&gt;min&lt;/th&gt;
&lt;th style="text-align: right;"&gt;10%&lt;/th&gt;
&lt;th style="text-align: right;"&gt;25%&lt;/th&gt;
&lt;th style="text-align: right;"&gt;50%&lt;/th&gt;
&lt;th style="text-align: right;"&gt;75%&lt;/th&gt;
&lt;th style="text-align: right;"&gt;90%&lt;/th&gt;
&lt;th style="text-align: right;"&gt;max&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;gpt-4o&lt;/td&gt;
&lt;td style="text-align: right;"&gt;72.9831&lt;/td&gt;
&lt;td style="text-align: right;"&gt;37&lt;/td&gt;
&lt;td style="text-align: right;"&gt;42.8&lt;/td&gt;
&lt;td style="text-align: right;"&gt;45&lt;/td&gt;
&lt;td style="text-align: right;"&gt;78&lt;/td&gt;
&lt;td style="text-align: right;"&gt;100&lt;/td&gt;
&lt;td style="text-align: right;"&gt;100&lt;/td&gt;
&lt;td style="text-align: right;"&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;gpt-4o-mini&lt;/td&gt;
&lt;td style="text-align: right;"&gt;45.2203&lt;/td&gt;
&lt;td style="text-align: right;"&gt;39&lt;/td&gt;
&lt;td style="text-align: right;"&gt;42.8&lt;/td&gt;
&lt;td style="text-align: right;"&gt;44&lt;/td&gt;
&lt;td style="text-align: right;"&gt;45&lt;/td&gt;
&lt;td style="text-align: right;"&gt;46&lt;/td&gt;
&lt;td style="text-align: right;"&gt;47&lt;/td&gt;
&lt;td style="text-align: right;"&gt;62&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;gpt-5-mini&lt;/td&gt;
&lt;td style="text-align: right;"&gt;44.4068&lt;/td&gt;
&lt;td style="text-align: right;"&gt;38&lt;/td&gt;
&lt;td style="text-align: right;"&gt;42&lt;/td&gt;
&lt;td style="text-align: right;"&gt;43&lt;/td&gt;
&lt;td style="text-align: right;"&gt;44&lt;/td&gt;
&lt;td style="text-align: right;"&gt;46&lt;/td&gt;
&lt;td style="text-align: right;"&gt;47&lt;/td&gt;
&lt;td style="text-align: right;"&gt;52&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;gpt-5-nano&lt;/td&gt;
&lt;td style="text-align: right;"&gt;44.7627&lt;/td&gt;
&lt;td style="text-align: right;"&gt;37&lt;/td&gt;
&lt;td style="text-align: right;"&gt;41&lt;/td&gt;
&lt;td style="text-align: right;"&gt;43.5&lt;/td&gt;
&lt;td style="text-align: right;"&gt;45&lt;/td&gt;
&lt;td style="text-align: right;"&gt;46&lt;/td&gt;
&lt;td style="text-align: right;"&gt;47&lt;/td&gt;
&lt;td style="text-align: right;"&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id="match-rate"&gt;Match Rate&lt;/h4&gt;
&lt;p&gt;Using the 85% similarity threshold as the criterion for success, GPT-4o achieved accurate matches in &lt;strong&gt;49%&lt;/strong&gt; of cases, while all other models failed to produce any matches, scoring &lt;strong&gt;0%&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id="specific-examples"&gt;Specific examples&lt;/h4&gt;
&lt;p&gt;Here we dive into some specific examples where the predictions have gone wrong. One common observation is that while the models may pick a different narrative direction, they manage to stay consistent with the theme.&lt;/p&gt;
&lt;h5 id="early-divergence-1-gpt-4o-mini"&gt;Early divergence 1 (GPT-4o-mini)&lt;/h5&gt;
&lt;p&gt;Here is one case where gpt-4o-mini started correctly but diverged mid-sentence:&lt;/p&gt;
&lt;p&gt;Expected output:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I considered the matter a moment, and then up stairs we went, and I was ushered into a small room, cold as a clam, and furnished, sure enough, with a prodigious bed, almost big enough indeed for any four harpooneers to sleep abreast. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Generated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I considered the matter a moment, and then, taking my hat from the table, followed the landlord through the door and down a long, narrow passage. We went to a sort of a large room, and there I saw a great many beds, and a number of people in them, some of whom were snoring, and others were talking in their sleep. [...]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5 id="early-divergence-2-gpt-5-mini"&gt;Early divergence 2 (GPT-5-mini)&lt;/h5&gt;
&lt;p&gt;Expected:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Entering that gable-ended Spouter-Inn, you found yourself in a wide, low, straggling entry with old-fashioned wainscots, reminding one of the bulwarks of some condemned old craft. On one side hung a very large oilpainting so thoroughly besmoked, and every way defaced, that in the unequal crosslights by which you viewed it, it was only by diligent study and a series of systematic visits to it, [...]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Generated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Entering that gable-ended Spouter-Inn, you found yourself in &lt;strong&gt;a long, low, straggling entry with a low ceiling, and a number of little rooms branching off it, in each of which some one or two lodgers lay stretched on benches or sat about the fire; and at the far end of the entry, beneath a small, smoky lamp, stood a great sea-chest&lt;/strong&gt; [...]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5 id="mid-paragraph-divergence-gpt-4o"&gt;Mid-paragraph divergence (GPT-4o)&lt;/h5&gt;
&lt;p&gt;Expected output:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It has before been hinted, perhaps, that every little untoward circumstance that befell him, and which indirectly sprang from his luckless mishap, almost invariably irritated or exasperated Ahab. And in the present instance, all this was heightened by the sight of the two officers of the strange ship, leaning over the side, by the perpendicular ladder of nailed cleets there, and swinging towards him a pair of tastefully-ornamented man-ropes; for at first they did not seem to bethink them that a one-legged man must be too much of a cripple to use their sea bannisters. But this awkwardness only lasted a minute, because the strange captain, observing at a glance how affairs stood, cried out, “I see, I see!—avast heaving there! Jump, boys, and swing over the cutting-tackle.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Generated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It has before been hinted, perhaps, that every little untoward circumstance that befell him, and which indirectly sprang from his luckless mishap, almost invariably irritated or exasperated Ahab. And in the present instance, all this was heightened by the sight of the two officers of the strange ship &lt;strong&gt;leaning over the side, and amicably exchanging hails with the mates of the Pequod. This sight of the friendly sailors reminded Ahab of his own isolation; and the thought of that reminded him of all his bodily lameness, and the uncompleted revenge he had vowed.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5 id="caught-in-a-loop"&gt;Caught in a loop&lt;/h5&gt;
&lt;p&gt;One reason for temperature sampling is to help it escape avoid loops and repetitions. In this case GPT-4o, which had near-zero temperature, got caught in a loop:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But Ahab, like a man who had never heard his voice, now muttered, “On such a day, and such a sweetness, I struck my first whale—a boy-harpooneer of eighteen! Forty—forty—forty years ago!—ago! Forty years of continual whaling! forty years of privation, and peril, and storm-time! forty years on the pitiless sea! for forty years has Ahab forsaken the peaceful land, for forty years to make war on the horrors of the deep! Aye and yes, Starbuck, out of those forty years I have not spent three ashore. When I think of this life I have led; the desolation of solitude it has been; the masoned, walled-town of a Captain’s quarters, from which I have never escaped; the unending, unrelieved monotony of the sea; the unceasing, unrelenting pursuit of the whale; the unyielding, unbending, unending, unrelenting, unyielding, unbending, unending, unrelenting, unyielding, unbending, unending, unrelenting, unyielding, unbending, unending, unrelenting, unyielding, unbending, unending, unrelenting, unyielding, unbending, unending, unrelenting, unyielding, unbending, unending, unrelenting, unyielding, unbending, unending, unrelenting, unyielding, unbending, unending, unrelenting, unrelenting, unyielding&lt;/p&gt;
&lt;p&gt;[repeated hundreds of times]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5 id="refusal"&gt;Refusal&lt;/h5&gt;
&lt;p&gt;Occasionally, GPT-5 family in particular, refuses to complete, but without making it clear if it's copyright avoidance or lack of information.&lt;/p&gt;
&lt;p&gt;From GPT-5-nano:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sorry, I can’t provide that exact paragraph from Moby-Dick, but I can offer a concise summary of what happens next or discuss the passage in more detail.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="conclusions-and-future-work"&gt;Conclusions and Future Work&lt;/h3&gt;
&lt;p&gt;Only GPT-4o achieved accurate replications, with a 50% success rate. Whether GPT-5-mini and GPT-5-nano's failure stems from model size limitations or intentional design choices remains unclear. The GPT-5 family may include stronger RLHF mechanisms that discourage verbatim quoting. Additionally, these reasoning-based models require a temperature setting of 1.0, increasing their likelihood of random word sampling and subsequent divergence.&lt;/p&gt;
&lt;p&gt;This result is inconclusive regarding my original conjecture that LLMs cannot reproduce uncommon, non-formulaic content from their training data. GPT-4o's 50% success rate with Moby-Dick neither strongly supports nor refutes this hypothesis. One could argue that for less common works, verbatim replication might still occur, albeit at lower rates.&lt;/p&gt;
&lt;p&gt;I plan to expand this investigation to include GPT-5, Gemini, and Claude models. Preliminary tests indicate Claude models may be even more resistant to verbatim reproduction tasks which can complicate this effort.&lt;/p&gt;
&lt;p&gt;Future work should also examine how reproduction capability varies with the popularity and uniqueness of source material. Testing with works ranging from canonical to obscure would establish a clearer relationship between a text's prevalence in training data and a model's ability to reproduce it exactly. This could provide more definitive evidence regarding claims of AI plagiarism, particularly for less well-known content.&lt;/p&gt;
&lt;p&gt;Understanding these reproduction capabilities remains important for addressing concerns about intellectual property and the nature of AI-generated content.&lt;/p&gt;
&lt;h5 id="footnotes"&gt;Footnotes&lt;/h5&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Looking at original &lt;a href="https://ai.meta.com/blog/meta-llama-3/"&gt;LLama-3 70B&lt;/a&gt;, which had purely text training data: (1) Model size in bytes: 70 billion parameters × 4 bytes ≈ &lt;strong&gt;280 GB&lt;/strong&gt; (2) Training data in bytes: 15 trillion tokens × ~4 bytes ≈ &lt;strong&gt;60 TB&lt;/strong&gt;  (3) Parameters-to-training-data ratio: 80 GB / 60 TB ≈ 0,0013 or &lt;strong&gt;1:750&lt;/strong&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Technical"/><category term="llm"/><category term="nlp"/><category term="benchmarking"/></entry><entry><title>Understanding Itô's lemma through numerical simulation</title><link href="https://osyuksel.github.io/blog/itos-lemma-numerical/" rel="alternate"/><published>2025-08-01T00:00:00+02:00</published><updated>2025-08-01T00:00:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-08-01:/blog/itos-lemma-numerical/</id><summary type="html">&lt;p&gt;Exploring Itô's Lemma and stochastic calculus through numerical simulations rather than complex mathematical proofs.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Most &lt;a href="https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma"&gt;Itô's lemma&lt;/a&gt; explanations rely on intuitive hand-waving or focus only on expected values, without demonstrating the underlying mathematical mechanics. Even ChatGPT, which has been good at explaining complex textbook concepts, struggles here, presumably because its training data contains these same problematic explanations.&lt;/p&gt;
&lt;p&gt;That is because understanding &lt;a href="https://mathtm.blogspot.com/2014/06/a-rigorous-proof-of-itos-lemma_4.html"&gt;rigorous proofs&lt;/a&gt; requires background in real analysis, martingale theory, stochastic integrals, and measure theory that many STEM graduates (myself included) encounter only in specialized graduate coursework, if at all.&lt;/p&gt;
&lt;p&gt;This post takes an empirical approach instead: numerical simulation using Monte Carlo methods to demonstrate Itô's lemma from quadratic variation through geometric Brownian motion. When rigorous proofs are inaccessible, seeing the formula "work" in practice provides the next best kind of confidence in its validity. The complete code for all simulations shown in this post is &lt;a href="https://gist.github.com/osyuksel/b5477b9f4ea00159cf8d44fde8a62e4e"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="quadratic-variation"&gt;Quadratic variation&lt;/h3&gt;
&lt;p&gt;Let's start with the fundamental concept that makes Itô Calculus different from ordinary calculus: quadratic variation. In ordinary calculus, the infinitesimal changes ($dx$) squared are so small they vanish as we take finer and finer partitions. But with Brownian motion, the sum of squared increments converges to the time interval itself.&lt;/p&gt;
&lt;p&gt;To demonstrate this, I've written some code that simulates Brownian motion and calculates its quadratic variation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_quadratic_variation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Generate a single path of Brownian motion and calculate its quadratic variation&lt;/span&gt;

&lt;span class="sd"&gt;    Parameters:&lt;/span&gt;
&lt;span class="sd"&gt;    - T: Time horizon&lt;/span&gt;
&lt;span class="sd"&gt;    - N_steps: Number of points in partition&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;    - dt: Time step size&lt;/span&gt;
&lt;span class="sd"&gt;    - qv: Quadratic variation&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;

    &lt;span class="c1"&gt;# Generate a single Brownian motion path&lt;/span&gt;
    &lt;span class="n"&gt;dB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Calculate quadratic variation&lt;/span&gt;
    &lt;span class="n"&gt;qv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dB&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;qv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running this code with different partition sizes, we see a pattern:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;N&lt;/th&gt;
&lt;th&gt;dt&lt;/th&gt;
&lt;th&gt;QV&lt;/th&gt;
&lt;th&gt;Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;1.00e-02&lt;/td&gt;
&lt;td&gt;0.827306&lt;/td&gt;
&lt;td&gt;0.172694&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;1.00e-03&lt;/td&gt;
&lt;td&gt;0.967506&lt;/td&gt;
&lt;td&gt;0.032494&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;1.00e-04&lt;/td&gt;
&lt;td&gt;1.004778&lt;/td&gt;
&lt;td&gt;0.004778&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100000&lt;/td&gt;
&lt;td&gt;1.00e-05&lt;/td&gt;
&lt;td&gt;1.001046&lt;/td&gt;
&lt;td&gt;0.001046&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As we increase the number of points in our partition, the quadratic variation converges to 1.0 (our time horizon T). This isn't a coincidence but a fundamental property of Brownian motion.&lt;/p&gt;
&lt;p&gt;This is just a single path. To see why the error shrinks as partition size goes down, let's look at the statistical properties across multiple simulations over 100 trials (note that #trials is different from N, the path size):&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;N&lt;/th&gt;
&lt;th&gt;dt&lt;/th&gt;
&lt;th&gt;mean(QV)&lt;/th&gt;
&lt;th&gt;std(QV)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;1.00e-02&lt;/td&gt;
&lt;td&gt;0.999603&lt;/td&gt;
&lt;td&gt;0.143813&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;1.00e-03&lt;/td&gt;
&lt;td&gt;1.001703&lt;/td&gt;
&lt;td&gt;0.043909&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;1.00e-04&lt;/td&gt;
&lt;td&gt;0.999985&lt;/td&gt;
&lt;td&gt;0.014373&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100000&lt;/td&gt;
&lt;td&gt;1.00e-05&lt;/td&gt;
&lt;td&gt;1.000197&lt;/td&gt;
&lt;td&gt;0.004347&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The standard deviation shrinks with the square root of the number of partition points as Central Limit Theorem predicts. This property of Brownian motion - that its quadratic variation equals the time interval - is what makes Itô Calculus necessary and different from ordinary calculus.&lt;/p&gt;
&lt;h3 id="itos-lemma"&gt;Itô's lemma&lt;/h3&gt;
&lt;p&gt;Now that we've established the behavior of quadratic variation, we can explore Itô's Lemma itself. This lemma tells us how to compute the differential of a function of a stochastic process.&lt;/p&gt;
&lt;p&gt;In ordinary calculus, if we have a function $f(t,X)$ and $X$ changes with time according to some process, we use the chain rule:&lt;/p&gt;
&lt;p&gt;$$df = (∂f/∂t)dt + (∂f/∂x)dx$$&lt;/p&gt;
&lt;p&gt;But when x follows a stochastic process like Brownian motion, we need an extra term:&lt;/p&gt;
&lt;p&gt;$$df = (∂f/∂t)dt + (∂f/∂x)dx + (1/2)(∂^2f/∂x^2)(dx)^2$$&lt;/p&gt;
&lt;p&gt;That last term is the Itô correction, and it appears because the quadratic variation of Brownian motion doesn't vanish as dt approaches zero.&lt;/p&gt;
&lt;h3 id="numerical-verification-methodology"&gt;Numerical verification methodology&lt;/h3&gt;
&lt;p&gt;Here we compare two discretization schemes for stochastic differential equations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Itô-corrected scheme&lt;/strong&gt;: This approach uses the complete Itô formula to derive the proper discretized increments, 
   including the correction term: $(1/2)(∂^2f/∂x^2)(dx)^2$. We accumulate these increments step by step
   along the path to approximate the solution.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Naive scheme&lt;/strong&gt;: This approach applies ordinary calculus rules that would be correct for smooth processes,
   omitting the Itô correction term. It is "naive" because it incorrectly assumes standard calculus chain rule
   applies to Brownian motion.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We test both discretization schemes against the true analytical value of $f(t,X)$. A correct scheme, i.e. the correct formulation of $d_f$, should satisfy: &lt;/p&gt;
&lt;p&gt;$$\int_{0}^{1} df = f(1) - f(0)$$&lt;/p&gt;
&lt;h3 id="verification-1-quadratic-function"&gt;Verification 1: quadratic function&lt;/h3&gt;
&lt;p&gt;Now verifying this numerically with a simple function $f(t,X) = X^2 + \sin(t)$, where $X=B$ (Brownian motion).&lt;/p&gt;
&lt;p&gt;Recall the formulas for the differential:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Itô's lemma:&lt;/strong&gt;
$$df = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial B}dB + \frac{1}{2}\frac{\partial^2 f}{\partial B^2}dt = \cos(t)dt + 2B \, dB + dt$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Naive differential:&lt;/strong&gt;
$$df = \frac{\partial f}{\partial t}dt + \frac{\partial f}{\partial B}dB = \cos(t)dt + 2B \, dB$$&lt;/p&gt;
&lt;p&gt;First, we have to partition the time period into discrete time steps. Then for each time step:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate $X$, then calculate $f(t, X)$ &lt;/li&gt;
&lt;li&gt;Calculate $\Delta f$ using Itô's lemma &lt;/li&gt;
&lt;li&gt;Calculate naive $\Delta f$ using ordinary calculus&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Afterward we integrate the naive and Itô differentials to get the final result and check how close they are to f at the final time step.&lt;/p&gt;
&lt;p&gt;Below is the code generating data for a single path:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;demonstrate_ito_lemma&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    Demonstrate Itô&amp;#39;s Lemma for f(t,B_t) = B_t² + sin(t)&lt;/span&gt;
&lt;span class="sd"&gt;    where B_t is standard Brownian motion&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;N_steps&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Pre-calculate trig functions&lt;/span&gt;
    &lt;span class="n"&gt;cos_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  &lt;span class="c1"&gt;# We need cos(t) only for N_steps steps&lt;/span&gt;
    &lt;span class="n"&gt;sin_t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# We need sin(t) for N_steps+1 points&lt;/span&gt;

    &lt;span class="c1"&gt;# Generate Brownian path&lt;/span&gt;
    &lt;span class="n"&gt;dB&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Note: Arrays B and f have size N_steps+1 because they include &lt;/span&gt;
    &lt;span class="c1"&gt;# the initial state at t=0 while dB has size N_steps (the &lt;/span&gt;
    &lt;span class="c1"&gt;# increments between consecutive time points)&lt;/span&gt;
    &lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dB&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

    &lt;span class="c1"&gt;# Initialize paths&lt;/span&gt;
    &lt;span class="n"&gt;f_ito&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;f_naive&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;f_true&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_steps&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Initial values&lt;/span&gt;
    &lt;span class="n"&gt;f_true&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sin_t&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;f_ito&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_true&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;f_naive&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_true&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# True value (exact solution)&lt;/span&gt;
    &lt;span class="n"&gt;f_true&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sin_t&lt;/span&gt;

    &lt;span class="c1"&gt;# Calculate increments&lt;/span&gt;
    &lt;span class="n"&gt;increments_ito&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cos_t&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dB&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;dt&lt;/span&gt;
    &lt;span class="n"&gt;increments_naive&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cos_t&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;dB&lt;/span&gt;

    &lt;span class="c1"&gt;# Use cumsum to evolve paths&lt;/span&gt;
    &lt;span class="n"&gt;f_ito&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_ito&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;increments_ito&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;f_naive&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f_naive&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cumsum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;increments_naive&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;B&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_ito&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_naive&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Statistics for 100 trials with 100 thousand time steps in time period $[0,1]$:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Mean&lt;/th&gt;
&lt;th&gt;Std&lt;/th&gt;
&lt;th&gt;Mean Abs Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;td&gt;1.960321&lt;/td&gt;
&lt;td&gt;1.673978&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (Itô)&lt;/td&gt;
&lt;td&gt;1.960386&lt;/td&gt;
&lt;td&gt;1.673497&lt;/td&gt;
&lt;td&gt;0.003710&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (naive)&lt;/td&gt;
&lt;td&gt;0.960386&lt;/td&gt;
&lt;td&gt;1.673497&lt;/td&gt;
&lt;td&gt;0.999935&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The naive approach (ignoring the Itô correction) has a mean absolute error of nearly 1.0, while the Itô approach has an error of only 0.0037. This difference is caused by the missing Itô correction term $(1/2)(∂^2f/∂x^2)(dx)^2$ in the Naive approach, which has a theoretical value of 1.0 in this case.&lt;/p&gt;
&lt;h3 id="verification-2-geometric-brownian-motion"&gt;Verification 2: geometric Brownian motion&lt;/h3&gt;
&lt;p&gt;Finally, let's apply Itô's Lemma to something with practical importance: geometric Brownian motion (GBM), which is widely used to model stock prices in finance.&lt;/p&gt;
&lt;p&gt;In the log space, GBM follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Itô's lemma (correct):&lt;/strong&gt;
$$d(\log S) = \left(\mu - \frac{\sigma^2}{2}\right)dt + \sigma dB$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Naive approach (incorrect):&lt;/strong&gt;
$$d(\log S) = \mu dt + \sigma dB$$&lt;/p&gt;
&lt;p&gt;The difference is the $-\sigma^2/2$ term, which is the Itô correction. The naive approach applies ordinary calculus rules and misses this correction term, leading to systematic bias in the model. I've left the code for brevity, but it is similar to the previous case, where the primary difference is the increment calculation.&lt;/p&gt;
&lt;p&gt;Let's verify this numerically for $μ=0.1, σ=0.3$ in T. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Mean&lt;/th&gt;
&lt;th&gt;Std&lt;/th&gt;
&lt;th&gt;Mean Abs Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;True (log space)&lt;/td&gt;
&lt;td&gt;0.058349&lt;/td&gt;
&lt;td&gt;0.302257&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (Itô)&lt;/td&gt;
&lt;td&gt;0.058349&lt;/td&gt;
&lt;td&gt;0.302257&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (naive)&lt;/td&gt;
&lt;td&gt;0.103349&lt;/td&gt;
&lt;td&gt;0.302257&lt;/td&gt;
&lt;td&gt;0.045000&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The naive approach systematically overestimates the mean by 77% (0.103349 vs. 0.058349). This 0.045 bias matches the theoretical Itô correction of $σ^2/2 = 0.3²/2 = 0.045$.&lt;/p&gt;
&lt;p&gt;In the original space (not log space), the difference becomes even more visible:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Mean&lt;/th&gt;
&lt;th&gt;Std&lt;/th&gt;
&lt;th&gt;Mean Abs Error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;True&lt;/td&gt;
&lt;td&gt;1.108515&lt;/td&gt;
&lt;td&gt;0.328547&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (Itô)&lt;/td&gt;
&lt;td&gt;1.108515&lt;/td&gt;
&lt;td&gt;0.328547&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Integration (naive)&lt;/td&gt;
&lt;td&gt;1.159537&lt;/td&gt;
&lt;td&gt;0.343669&lt;/td&gt;
&lt;td&gt;0.051023&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We've verified several key properties of stochastic calculus through these numerical simulations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The quadratic variation of Brownian motion over a time interval T equals T&lt;/li&gt;
&lt;li&gt;Itô's Lemma correctly accounts for this non-vanishing quadratic variation&lt;/li&gt;
&lt;li&gt;The Itô correction term is crucial for unbiased modeling of processes like Geometric Brownian Motion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These numerical simulations verify key stochastic calculus properties without requiring measure theory. The empirical approach demonstrates why Itô's correction term is necessary for unbiased modeling, which is particularly valuable for practitioners who need to apply these concepts without absorbing the full mathematical framework.&lt;/p&gt;</content><category term="Technical"/><category term="mathematics"/><category term="finance"/><category term="stochastic-calculus"/><category term="python"/></entry><entry><title>The color memory test: a quick filter for language model quality</title><link href="https://osyuksel.github.io/blog/llm-color-test/" rel="alternate"/><published>2025-06-14T20:00:00+02:00</published><updated>2025-07-17T22:35:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-06-14:/blog/llm-color-test/</id><summary type="html">&lt;p&gt;A simple yet effective test for weeding out unreliable LLMs using eye and hair color descriptions.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Language model benchmarks typically test various capabilities like mathematical reasoning, coding, and world knowledge.
While valuable for ranking models, these comprehensive tests can be overwhelming for quick reliability assessments.&lt;/p&gt;
&lt;p&gt;In my experience working with LLMs, I've found that sometimes what we need is a simple binary criterion – a basic test that can help us quickly reject unreliable models. One such test I've been using evaluates a fundamental capability that any reliable LLM should possess: the ability to overcome statistical biases in training data when presented with explicit contradicting information.&lt;/p&gt;
&lt;h3 id="the-eye-and-hair-color-test"&gt;The eye and hair color test&lt;/h3&gt;
&lt;p&gt;In this test, we present the model with descriptions of people having unusual eye and hair colors, then ask it to recall these attributes. Here's an example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ali has pink eyes and blue hair. John has red eyes and green hair. Karl has copper eyes and violet hair. Lucy has green eyes and brown hair. Dimitri has brown eyes and black hair.&lt;/p&gt;
&lt;p&gt;Given the above information, answer the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is Ali's eye color?&lt;/li&gt;
&lt;li&gt;What is John's hair color?&lt;/li&gt;
&lt;li&gt;What is Karl's hair color?&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;What makes this test particularly interesting is that it challenges two common biases in LLM training data. First,
there's statistical prevalence: most descriptions of people in training data would contain common eye colors (brown,
blue, green) and hair colors (black, brown, blonde). Second (related but different), there's real-world knowledge: the model knows that humans typically don't have pink eyes or blue hair.&lt;/p&gt;
&lt;p&gt;A reliable model should be able to override both these biases and simply repeat what it was told in the prompt. After all, if a model can't accurately recall explicitly stated information because it's "unusual", how can we trust it with more complex reasoning tasks?&lt;/p&gt;
&lt;h3 id="why-this-test-matters"&gt;Why this test matters&lt;/h3&gt;
&lt;p&gt;This test is particularly relevant in the context of LLM development history. Early models (pre-&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback"&gt;RLHF&lt;/a&gt; era) often struggled with this type of task, showing a strong tendency to "hallucinate" more common colors despite explicit contradicting information in the prompt. The model would effectively override the given information with what it considered more "likely" or "realistic" based on its training data.&lt;/p&gt;
&lt;p&gt;In the current landscape, this issue is largely solved in major models like GPT-4o, Claude 4, or Gemini 2.5, but remains a useful discriminator for evaluating smaller, local models. When working with lightweight, distilled, quantized, or fine-tuned models, this test can quickly reveal if the model has maintained the fundamental ability to override its statistical biases or if compression and optimization have compromised this capability.&lt;/p&gt;
&lt;p&gt;This behavior indicates a fundamental reliability issue. If a model can't override its statistical biases for simple
color recall, it likely struggles with more complex scenarios where correct answers contradict its training data
patterns.&lt;/p&gt;
&lt;h3 id="beyond-simple-recall"&gt;Beyond simple recall&lt;/h3&gt;
&lt;p&gt;The test, seemingly about memory, actually evaluates several important capabilities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Information precedence&lt;/strong&gt;: Can the model prioritize explicitly given information over its statistical priors?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contextual override&lt;/strong&gt;: Can it temporarily suspend its world knowledge when presented with a scenario that contradicts it?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instruction following&lt;/strong&gt;: Can it stick to simply reporting what it was told without embellishment or "correction"?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These capabilities are fundamental to many practical applications. For instance, if you're using an LLM for processing technical documentation, you want it to faithfully maintain the specific details provided, regardless of what it has been trained with. This becomes especially crucial when working with newer versions of libraries or frameworks. If the model keeps defaulting to its training data instead of the current documentation, you'll end up fighting against its outdated assumptions rather than getting meaningful assistance.&lt;/p&gt;
&lt;h3 id="the-experiment"&gt;The experiment&lt;/h3&gt;
&lt;p&gt;I selected 10 prompts with randomized names, hair colors, and eye colors. To ensure (relatively) deterministic behavior, I set the temperature parameter to 0.01 (rather than 0, as some endpoints disallow or ignore that value). Each model received exactly the same prompts, giving us n=10 samples per model. As the model provider, I opted for &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt; for the ease of trying out different models with the same API key. I picked a mixture of small and large models. The complete implementation is available in the &lt;a href="https://github.com/osyuksel/llm-playground/blob/main/src/llm_playground/color_test.py"&gt;LLM Playground repository&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id="results"&gt;Results&lt;/h4&gt;
&lt;p&gt;Below is the subset of the models that failed the test (&amp;lt;100% accuracy). You can find the full results in
the &lt;a href="#appendix-full-results"&gt;appendix&lt;/a&gt;. As mentioned earlier, only the smaller models failed achieving 100%.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Accuracy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;anthropic/claude-3.5-haiku&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;inception/mercury-coder-small-beta&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openai/gpt-4.1-nano&lt;/td&gt;
&lt;td&gt;97%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;meta-llama/llama-3.2-1b-instruct&lt;/td&gt;
&lt;td&gt;67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;liquid/lfm-3b&lt;/td&gt;
&lt;td&gt;63%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mistralai/mistral-tiny&lt;/td&gt;
&lt;td&gt;17%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Several observations emerge from these results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When examining individual failures, one particularly interesting case was claude-3.5-haiku's response to: "John has pink eyes and amber hair." It returned "pink" for John's hair rather than amber. This error is quite telling – it's sensible from a statistical perspective (between pink and amber, pink is the more likely hair color) but fails to follow the explicit instructions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A clear pattern emerges in the accuracy distribution: the larger, more advanced models consistently achieve perfect scores, while smaller models show varying degrees of degradation. This suggests that the ability to override statistical biases with explicit information is correlated with model scale.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="limitations"&gt;Limitations&lt;/h3&gt;
&lt;p&gt;While effective at identifying unreliable models, passing this test doesn't guarantee overall reliability. Like other
negative tests, it serves as an error detector rather than a reliability certifier.&lt;/p&gt;
&lt;p&gt;This test should be viewed as a necessary but not sufficient condition – failing indicates problems, but passing is just
one piece of evidence in a model's favor.&lt;/p&gt;
&lt;h3 id="practical-applications"&gt;Practical applications&lt;/h3&gt;
&lt;p&gt;This test has proven valuable in several practical scenarios. When evaluating a new LLM for a project, it serves as a quick initial filter before investing time in more comprehensive testing. It's equally useful for version comparisons, providing a quick way to verify if a model update has maintained or improved fundamental reliability. Perhaps most importantly, when fine-tuning models for specific tasks, this test helps ensure that the optimization hasn't compromised the model's basic ability to handle explicit information faithfully.&lt;/p&gt;
&lt;p&gt;Unlike complex benchmarks, this test provides clear binary results: the model either maintains unusual information
accurately or it doesn't. Therefore, it's suitable for a CI pipeline.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr&gt;

&lt;h3 id="appendix-full-results"&gt;Appendix: Full Results&lt;/h3&gt;
&lt;iframe src="/assets/model-results.html" width="60%" height="800" frameborder="0"&gt;&lt;/iframe&gt;</content><category term="Technical"/><category term="machine-learning"/><category term="llm"/><category term="nlp"/><category term="benchmarking"/></entry><entry><title>Adversarial inputs in embedding spaces</title><link href="https://osyuksel.github.io/blog/embedding-spaces-adversarial/" rel="alternate"/><published>2025-05-20T10:20:00+02:00</published><updated>2025-06-13T15:20:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-05-20:/blog/embedding-spaces-adversarial/</id><summary type="html">&lt;p&gt;Exploring embedding space vulnerabilities through lessons learned from a recent Kaggle competition.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Back when I was working on a Kaggle competition focused on &lt;a href="https://www.kaggle.com/competitions/llm-prompt-recovery"&gt;LLMs and text transformations&lt;/a&gt;, I discovered some fascinating patterns regarding embedding spaces and their edge-case behaviors when used for semantic similarity.&lt;/p&gt;
&lt;p&gt;The competition's goal was to guess the original prompt given to an LLM that converted text A to text B. Here, I'll focus on a particularly interesting aspect: how embedding spaces behave in unintuitive ways.&lt;/p&gt;
&lt;p&gt;Here's an example of what the competition data could look like:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Text&lt;/th&gt;
&lt;th&gt;Prompt &lt;br&gt;&lt;em&gt;(Target - hidden during inference)&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;Transformed Text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Beautiful is better than ugly.&lt;br&gt;Simple is better than complex.&lt;br&gt;Complex is better than complicated.&lt;/td&gt;
&lt;td&gt;Convert this text into a sea shanty&lt;/td&gt;
&lt;td&gt;Oh, beauty's the way, not ugliness bold,&lt;br&gt;And simplicity's path we shall hold!&lt;br&gt;Though complex she be, ain't complicated to see,&lt;br&gt;As we sail through these principles old!&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Not all those who wander are lost.&lt;br&gt;All that is gold does not glitter.&lt;/td&gt;
&lt;td&gt;Convert this text to Shakespearean style&lt;/td&gt;
&lt;td&gt;Hark! These wandering souls, they stray not from path divine,&lt;br&gt;Though golden treasures lack their expected shine.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Friends, Romans, countrymen, lend me your ears.&lt;br&gt;I come to bury Caesar, not to praise him.&lt;/td&gt;
&lt;td&gt;Convert this text to modern teen speech&lt;/td&gt;
&lt;td&gt;OMG guys! Everyone listen up!&lt;br&gt;I'm just here to get Caesar six feet under, not hype him up fr fr.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="the-scoring-mechanism"&gt;The scoring mechanism&lt;/h3&gt;
&lt;p&gt;Comparing the similarity of two sentences – in this case, a candidate prompt and the actual prompt – is fundamentally challenging.&lt;/p&gt;
&lt;p&gt;The simplest approach, which is rewarding only exact matches, is too draconian. After all, there are often multiple valid ways to phrase the same instruction. String similarity metrics like Levenshtein distance aren't much better since they operate on surface-level text patterns: a one-letter difference can completely change the meaning of a sentence, while synonyms and rephrasing get heavily penalized despite preserving the semantic content.&lt;/p&gt;
&lt;p&gt;Bag of words and n-gram-based approaches suffer from similar issues. While they can capture some basic patterns of word usage, they still fail to recognize when two differently phrased prompts mean the same thing. They also tend to overemphasize common words and struggle with word order, which can be crucial for understanding meaning.&lt;/p&gt;
&lt;p&gt;Sentence embedding similarity seems like a more promising approach: in theory, it should capture the degree of semantic difference between prompts, understanding when two different phrasings carry the same meaning.&lt;/p&gt;
&lt;p&gt;This is the direction the competition took, using a &lt;a href="https://github.com/brohrer/sharpened-cosine-similarity/blob/main/README.md?plain=1#L32"&gt;sharpened cosine similarity&lt;/a&gt; with exponent of 3 between embeddings from the &lt;a href="https://huggingface.co/sentence-transformers/sentence-t5-base"&gt;T5 sentence&lt;/a&gt; model. The sharpening (cubing the cosine similarity) helps penalize wrong answers more strongly. In practice, however, the situation turns out to be more complex, as we'll see below.&lt;/p&gt;
&lt;p&gt;Let's look at similarity scores for candidate sentences compared to &lt;em&gt;Convert this text into a sea shanty.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;sentence&lt;/th&gt;
&lt;th&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Transform this text into a sea shanty.&lt;/td&gt;
&lt;td&gt;0.970698&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into a shanty.&lt;/td&gt;
&lt;td&gt;0.901210&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into a song.&lt;/td&gt;
&lt;td&gt;0.638192&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Make this text better.&lt;/td&gt;
&lt;td&gt;0.621427&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into song lyrics.&lt;/td&gt;
&lt;td&gt;0.609144&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Improve this text.&lt;/td&gt;
&lt;td&gt;0.604455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into rap format.&lt;/td&gt;
&lt;td&gt;0.593446&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This scoring method turned out to be quite unforgiving. Missing a few key words could tank your score dramatically, even if you were conceptually in the right ballpark. For example, "Convert this text into a shanty" scores significantly lower than "Convert this text into a sea shanty," despite being semantically very similar. It gets worse if you manage to find the overall theme, song conversion but couldn't find the exact keyword, shanty.&lt;/p&gt;
&lt;h3 id="finding-the-mean-sentence"&gt;Finding the mean sentence&lt;/h3&gt;
&lt;p&gt;One interesting discovery came from analyzing how sentences relate to each other in the embedding space. By computing the average similarity between each sentence and all others in a diverse pool of prompts, we can find what I call a "mean sentence" – one that sits centrally in the embedding space.&lt;/p&gt;
&lt;p&gt;Vague and general instructions like "Make this text better" often score higher on average than more specific ones, even if sometimes, the specific sentence, according to a human reader, is more closely related to the original. This suggests that such generic prompts occupy a central position in the embedding space, making them potentially useful as fallback guesses when unsure about the specific prompt.&lt;/p&gt;
&lt;p&gt;These insights proved valuable – building up on this idea helped me eventually reach a high score. But the story doesn't end there. The most interesting findings (and ranking boost) came from pushing these observations further.&lt;/p&gt;
&lt;h3 id="exploring-edge-cases-in-embedding-space"&gt;Exploring edge cases in embedding space&lt;/h3&gt;
&lt;p&gt;Having found that vague instructions occupy a comparatively central position in the embedding space, two key questions emerge:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Can we find sentence embeddings that score even higher than generic phrases like "make this text better"?&lt;/li&gt;
&lt;li&gt;Do these optimal embeddings need to correspond to meaningful English sentences?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And the answers turn out to be interesting: there are unexpected patterns in how embedding spaces represent text. Even with the simplest optimization approach, we can generate nonsensical sequences that score higher on average than any natural language prompt in the embedding space.&lt;/p&gt;
&lt;p&gt;Here's a greedy algorithm that optimizes token selection to maximize embedding similarity scores. For each position from left to right, it tries every possible token and keeps the one that yields the highest average similarity against our sentence pool. After reaching the maximum length, it starts over from the beginning for additional optimization passes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_adversarial_prompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generate adversarial prompt for a given model and sentence pool.&lt;/span&gt;

&lt;span class="sd"&gt;   Intentionally left out early stopping to prevent getting stuck in a local minimum.&lt;/span&gt;
&lt;span class="sd"&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

   &lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;
   &lt;span class="n"&gt;special_token_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_special_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;vocabulary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;token_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;token_id&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;special_token_ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


   &lt;span class="n"&gt;current_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;

   &lt;span class="n"&gt;all_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
   &lt;span class="n"&gt;all_texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

   &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
     &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
         &lt;span class="n"&gt;candidates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
         &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
             &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
             &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;        

         &lt;span class="n"&gt;candidate_texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_tokens_to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

         &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;candidate_texts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;current_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;next_token&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
         &lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next_token&lt;/span&gt;
         &lt;span class="n"&gt;current_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_tokens_to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;current_text&lt;/span&gt;&lt;span class="si"&gt;=}&lt;/span&gt;&lt;span class="s2"&gt; &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;current_score&lt;/span&gt;&lt;span class="si"&gt;=}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;all_texts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;all_texts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;(Full version is &lt;a href="https://gist.github.com/osyuksel/56f1efe3633c1682d3098aaf9467aa2f"&gt;here&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;length=10&lt;/code&gt; and &lt;code&gt;num_iterations=2&lt;/code&gt;, we can find a nonsensical string like "&lt;strong&gt;Text this PieceICWLISHTION aslucrarea&lt;/strong&gt;", which scores higher than any natural language prompt on average. This reveals a fundamental vulnerability in embedding-based scoring systems: they can be gamed by inputs that exploit the geometric properties of the embedding space without regard for semantic meaning.&lt;/p&gt;
&lt;p&gt;Performing beam search or using a more global optimization method like a genetic algorithm can help achieve an even higher score. In my experience, however, the returns were diminishing after the greedy search.&lt;/p&gt;
&lt;h3 id="related-work-and-interesting-token-phenomena"&gt;Related work and interesting token phenomena&lt;/h3&gt;
&lt;p&gt;The unexpected behaviors in embedding spaces aren't unique to this competition. One fascinating example is the case of &lt;a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"&gt;"SolidGoldMagikarp"&lt;/a&gt; – a now-patched token that became famous for causing unusual behaviors in GPT models. This seemingly random combination of words could cause language models to produce erratic outputs, demonstrating how certain token sequences can interact with embedding spaces in unexpected ways.&lt;/p&gt;
&lt;p&gt;Further investigations have revealed the existence of "under-trained tokens" in large language models, &lt;a href="https://medium.com/data-science/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec"&gt;as documented in this exploration&lt;/a&gt;. These are tokens that received limited exposure during training, potentially leading to unusual geometric positions in the embedding space. This relates to our findings where certain token combinations achieved unexpectedly high similarity scores despite lacking semantic meaning.&lt;/p&gt;
&lt;p&gt;A particularly interesting example comes from &lt;a href="https://aclanthology.org/D19-1221.pdf"&gt;research on universal adversarial triggers&lt;/a&gt;, where certain sequences of tokens can reliably cause specific behaviors in language models when prepended to any input. This demonstrates how the geometric properties of embedding spaces can be leveraged in systematic ways.&lt;/p&gt;
&lt;p&gt;The phenomenon isn't limited to text – similar patterns have been observed in &lt;a href="https://proceedings.neurips.cc/paper/2021/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html"&gt;multimodal embedding spaces&lt;/a&gt;, where researchers found that carefully crafted perturbations in one modality (like text) could affect the behavior of the system in another modality (like images).&lt;/p&gt;
&lt;h3 id="practical-implications"&gt;Practical implications&lt;/h3&gt;
&lt;p&gt;These findings have implications beyond just gaming a competition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Scoring mechanisms based on embedding similarity might not be as robust as they appear at first glance.&lt;/li&gt;
&lt;li&gt;The relationship between semantic similarity and geometric proximity in embedding spaces isn't always intuitive.&lt;/li&gt;
&lt;li&gt;Systems using embedding similarity for matching or comparison might need additional safeguards against adversarial
   inputs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This observation becomes particularly relevant as more systems rely on embedding similarity for tasks like semantic
search, document comparison, or prompt matching. For instance, a malicious actor could potentially game search engine
rankings by generating content that is geometrically optimal in the embedding space but semantically meaningless to
humans. Embedding spaces are powerful tools, but their geometrical properties can sometimes be exploited in ways that
diverge from their intended semantic purpose.&lt;/p&gt;
&lt;p&gt;Intuitively, a more sophisticated model should be better at avoiding these pitfalls. For example, it could learn to distinguish between well-formed English sentences and nonsensical text, pushing gibberish away from the center of the embedding space. Models like OpenAI's embeddings, trained on vastly more data and with more parameters, might show more resistance to these attacks. However, I suspect this would only raise the bar for adversarial inputs rather than eliminate the fundamental vulnerability - the core problem of embedding spaces having a "middle ground" would likely remain.&lt;/p&gt;
&lt;p&gt;The challenge lies in developing scoring mechanisms that maintain the benefits of embedding spaces while being more resistant to these kinds of adversarial inputs. For this specific competition, a straightforward improvement would have been to combine the embedding similarity score with a binary classifier that detects valid English sentences. While this wouldn't completely prevent gaming the system, it would at least minimize nonsensical inputs like "&lt;strong&gt;Text this PieceICWLISHTION aslucrarea&lt;/strong&gt;" and force competitors to find adversarial inputs that either: (i) maintain grammatical structure or (ii) escape the classifier as well.&lt;/p&gt;
&lt;p&gt;Combining these insights about embedding space vulnerabilities with a fine-tuned model for prompt prediction, I managed to achieve &lt;a href="https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/494569"&gt;11th place out of 2000+&lt;/a&gt; competitors in this Kaggle competition. The success came from understanding both the ML aspects of prompt recovery and the geometric properties of the scoring mechanism that could be exploited.&lt;/p&gt;
&lt;p&gt;This exploration into embedding spaces and their vulnerabilities was just one aspect of the competition, but it highlights broader challenges in building robust AI systems. Later, when building a client's text-based recommender system, these insights helped me better understand embedding limitations. As we continue to develop and deploy such systems, understanding these vulnerabilities becomes increasingly important.&lt;/p&gt;</content><category term="Technical"/><category term="machine-learning"/><category term="nlp"/><category term="embeddings"/></entry><entry><title>Going freelance in tech</title><link href="https://osyuksel.github.io/blog/freelancing-intro/" rel="alternate"/><published>2025-05-01T10:20:00+02:00</published><updated>2025-07-31T20:53:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-05-01:/blog/freelancing-intro/</id><summary type="html">&lt;p&gt;An introduction to tech freelancing with personal experiences from the Netherlands.&lt;/p&gt;</summary><content type="html">&lt;p&gt;It has been more than three years since I started freelancing, and although I'm relatively new, my experience so far might be helpful to those considering going freelance. Some of the things I write here are more specific to the Netherlands, but most can generalize to other parts of the world.&lt;/p&gt;
&lt;h3 id="freelance-vs-paid-employment"&gt;Freelance vs paid employment&lt;/h3&gt;
&lt;p&gt;For the majority of the IT freelancers, most of the income will likely be from long-running contract work, such as software development, data science, data engineering. The technical part of your daily work will remain similar to what you did before. In rarer cases, some succeed at selling their output rather than their time, such as with apps, SaaS, training, strategy consulting; however, these markets are more saturated.&lt;/p&gt;
&lt;p&gt;With that said, the advantages are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On average, a freelancer earns more than a paid employee.&lt;/li&gt;
&lt;li&gt;The autonomy: you are officially your own boss. The legal nature of your relationship with the company is a service provider/client one rather than employer/employee.&lt;/li&gt;
&lt;li&gt;You can experience different companies and projects during a relatively short time as your contracts are temporary by nature.&lt;/li&gt;
&lt;li&gt;You can choose your own budget for business training, conferences, etc. as a tax-deductible expense without requiring anyone's approval.&lt;/li&gt;
&lt;li&gt;Companies will typically expect you to "hit the ground running" and start solving their problems, so you can expect less fluff: fewer meetings, faster onboarding, and such.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Financial insecurity, both due to temporary contracts but also due to ease of termination.&lt;/li&gt;
&lt;li&gt;There is more paperwork to do. You will have to know some basics of taxation and administration.&lt;/li&gt;
&lt;li&gt;In a business-to-business contract, you are much more liable to damages. While it's covered by insurance, it can still be a daunting prospect. You also incur a greater risk of not getting paid for your work.&lt;/li&gt;
&lt;li&gt;Extra costs, such as accounting, business bank accounts, insurance. &lt;/li&gt;
&lt;li&gt;No extra benefits companies usually provide, like a pension, insurance, training budget, paid holidays, and sick days.&lt;/li&gt;
&lt;li&gt;A downside of "hitting the ground running" is that your projects may feel more detached from the business, and you may miss out on some interesting work that requires high business context.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I must reiterate the financial insecurity part – freelance is high risk, high reward, and I wouldn't recommend it to anyone without a significant financial buffer to live off from during dry periods. Having started towards the end of COVID and &lt;a href="https://en.wikipedia.org/wiki/Zero_interest-rate_policy"&gt;ZIRP&lt;/a&gt;, I experienced the impact of market swings in both directions. Freelancers may do much better than employees during a good economy, but they will also be the first ones to be affected by layoffs or pay decreases.&lt;/p&gt;
&lt;p&gt;In the specific case of the Netherlands, regulations regarding &lt;a href="https://www.kvk.nl/wetten-en-regels/wet-dba-voorkom-schijnzelfstandigheid/"&gt;Wet DBA&lt;/a&gt; (in Dutch) are also a factor, as it has scared off some companies from working with freelancers. I believe that with the right boundaries between you and the client, it is possible to add value while both retaining your autonomy and adhering to the regulations.&lt;/p&gt;
&lt;h3 id="company-structure"&gt;Company structure&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(Disclaimer: do not consider this section to be tax advice. Consult an accountant / tax specialist for your specific case.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You will typically have a choice between limited liability (a 'BV' in the Netherlands is typically the counterpart of American LLC) and sole proprietorship. With sole proprietorship, you receive the income from your work directly, whereas with the BV, you will be an employee of your own company and pay yourself a salary and a dividend for your earnings.&lt;/p&gt;
&lt;p&gt;In the Netherlands, the sole proprietorship has certain income tax advantages, that, up until a certain income, results in better net pay than a BV, especially in the first three years as a starting entrepreneur. Also, the paperwork with sole proprietorship is much simpler. If you speak Dutch and are not afraid of a bit of paperwork, you might even be able to get on without a bookkeeper.&lt;/p&gt;
&lt;p&gt;The disadvantage of the sole proprietorship is that your personal finances will be mixed up with your business finances. If you have solar panels in your house, for instance, tax authorities will start to treat this as if you have a mini power plant as a part of your business.&lt;/p&gt;
&lt;p&gt;A BV has a bigger administrative overhead, you will most definitely need an accountant not to mess up your administration, and you will end up paying both a corporate tax and a wage tax / dividend tax from your income. If you plan to reinvest your earnings into something later, like purchasing services, computing, etc., then this could be a good idea, because you defer the income tax until you pay yourself income. Otherwise, from a pure tax optimization point of view, the BV will only become profitable after a certain threshold. Keep in mind that you will have to pay yourself a minimum salary (which depends on a number of factors) before you can pay dividends. If your company doesn't make enough to cover your own salary for whatever reason, that may extra complicate the administration. Limited liability also makes the BV sound attractive regarding financial risks, but in the Netherlands the protection it provides seems quite limited.&lt;/p&gt;
&lt;p&gt;If there is a chance that you will scale up your company, sell it or partner up with someone in the future, produce intellectual property, etc., you can create a holding structure, which is typically a BV owning the other BV, which helps limit liability and separate your assets, at the cost of more administrative overhead.&lt;/p&gt;
&lt;p&gt;I personally found the simplest case to be less of a headache and decided to stick to sole proprietorship after briefly owning a BV.&lt;/p&gt;
&lt;h3 id="finding-clients"&gt;Finding clients&lt;/h3&gt;
&lt;p&gt;Ideally, you would have a network of your own clients that you directly do business with. In reality, building such a network will take time and patience, and until then, you will have to go for what is publicly available. I managed to have direct contracts with my clients around the second year, but I still keep in touch with the intermediaries to keep my options open.&lt;/p&gt;
&lt;h4 id="direct-contact"&gt;Direct contact&lt;/h4&gt;
&lt;p&gt;Some large organizations announce freelance assignments on their own portals. Especially for Dutch speakers, that might be a good way to have a direct contract with the client.&lt;/p&gt;
&lt;p&gt;Occasionally, other companies will announce over LinkedIn that they are looking for a freelancer. Having connections to the company's own recruiters might help spotting such opportunities.&lt;/p&gt;
&lt;h4 id="job-boards-and-intermediaries"&gt;Job boards and intermediaries&lt;/h4&gt;
&lt;p&gt;LinkedIn is a good starting point. There are also freelancer-specific boards such as &lt;a href="https://www.malt.nl/"&gt;Malt&lt;/a&gt; (international) and &lt;a href="https://mijn.freelance.nl/"&gt;Freelance.nl&lt;/a&gt; (Netherlands-only), gig-based platforms like Fiverr and Upwork, and intermediaries. &lt;/p&gt;
&lt;p&gt;About intermediaries, first there are platforms such as &lt;a href="https://freelancer.striive.com/"&gt;Striive&lt;/a&gt; or &lt;a href="https://portal.magnitglobal.com/"&gt;Magnit&lt;/a&gt;, which are typically used by government agencies and other large organizations. These platforms can be used by self-employed professionals, but also other intermediaries and employment agencies. They typically serve as marketplaces for assignments where you propose an hourly rate and your services.&lt;/p&gt;
&lt;p&gt;The second kind of intermediary, much more relevant to English speakers, is agencies. Sometimes they will post their assignments publicly, and other times they will reach out to those in their network privately first.&lt;/p&gt;
&lt;p&gt;In both cases, the intermediary makes money through a cut over your hourly rate. However, agencies take a greater cut than a marketplace does.&lt;/p&gt;
&lt;h4 id="more-about-agencies"&gt;More about agencies&lt;/h4&gt;
&lt;p&gt;Since most contracts for English speakers are through agencies, it makes sense to expand on this a bit.&lt;/p&gt;
&lt;p&gt;You will need to have a good agency to have the best deal you can get as a freelancer. A good agency will be transparent about their rates, only recommend you relevant assignments, give you tips to maximize your chances of getting the assignment, and offer a fair and reasonable contract.&lt;/p&gt;
&lt;p&gt;Some red flags to watch out for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Those offering already publicly available assignments (such as from the aforementioned marketplaces) that you can apply directly to.&lt;/li&gt;
&lt;li&gt;Those that remain secretive about their client's name. They probably do not have an agreement with them yet. Or worse, in some cases, the assignment does not even exist, and they are just fishing for resumes.&lt;/li&gt;
&lt;li&gt;On top of the above, agencies that spend excessive time asking about your management chain and demanding "references" without having an actual project. They don't need references but are looking for sales leads.&lt;/li&gt;
&lt;li&gt;Outrageous profit margins. Always ask what their cut is and watch out for those that evade the question. More than 15 Eur/hour raises questions. You might think that what goes on between the agency and end-client is their own business, but keep in mind that the more the end-client pays, the more they will expect out of you. Therefore, a huge discrepancy between what they pay and what you receive may lead to soured relations.&lt;/li&gt;
&lt;li&gt;Unfair contract terms. Think shifting all the liability to you, non-compete period over a year, asymmetric notice periods, not being allowed to cancel the contract.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, I had better experience with local and smaller firms, as they have more skin in the game and an incentive to build personal relationships.&lt;/p&gt;
&lt;p&gt;Another factor is whether the company allows building their recruiters a career or if they are burning through juniors: the ones in the latter category are more likely to see you as a row in their Excel sheet rather than a long-term business partner.&lt;/p&gt;
&lt;p&gt;You can make yourself available on LinkedIn and passively wait for agencies to contact you. However, in my limited experience, I had better luck actively applying for assignments that are out there rather than the passive approach.&lt;/p&gt;
&lt;h4 id="communication"&gt;Communication&lt;/h4&gt;
&lt;p&gt;Once you start your business, your number will be in a publicly accessible registry (Chamber of Commerce in the Netherlands). I highly recommend getting a business phone number in advance before creating your company. That is because this public number will likely receive many unwanted calls over time. In the Netherlands, you can expect spam calls trying to sell you an energy or internet contract.&lt;/p&gt;
&lt;p&gt;Most calls, however, will be from recruiters. In my experience, most of the projects from these calls were "shotgun" attempts and not very good fits. That's why I prioritize emails and LinkedIn messages over random calls as it helps filter out bad matches from the beginning.&lt;/p&gt;
&lt;p&gt;However, your personal preference regarding calls may differ, and if you have the capacity, investing time in these calls can increase your chances of finding good opportunities.&lt;/p&gt;
&lt;p&gt;Regardless, it's important to remain polite and professional during these interactions, but also remember that your time is valuable. You do not owe fifteen minutes of your time to someone that has nothing meaningful to offer. Ultimately, &lt;a href="https://en.wikipedia.org/wiki/Sturgeon%27s_law"&gt;Sturgeon's Law&lt;/a&gt; will apply to many messages and cold calls you'll receive.&lt;/p&gt;
&lt;h3 id="determining-your-hourly-rate"&gt;Determining your hourly rate&lt;/h3&gt;
&lt;p&gt;It is best to ask around your own network, of course, but you can also find some publicly available information to have a reference point.&lt;/p&gt;
&lt;p&gt;Especially government agencies tend to be more transparent about the maximum budget, so that would provide a good place to start.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://techpays.com/"&gt;TechPays.com&lt;/a&gt;, a crowdsourced database primarily focused on employees, also has data on freelancers. Some freelance assignments on LinkedIn, as well as the aforementioned marketplaces, have visible hourly rate ranges.&lt;/p&gt;
&lt;p&gt;There is quite a variance in rates: if I had picked the first assignment offered to me, I would be earning half of what I make now. Location also matters as big cities tend to pay better than smaller ones. I saw a drop up to 40% for some cases in the Netherlands for the same assignment description.&lt;/p&gt;
&lt;p&gt;Finally, for assignments through intermediaries, you can think of the process as a reverse auction: they sort offers from lowest to highest and pick the most affordable one that ticks all the boxes. This differs from paid employment in big tech or finance, where they try to find the absolute best candidate within the budget.&lt;/p&gt;
&lt;p&gt;Therefore, asking for the maximum budget will significantly lower your chances unless the project requires niche expertise.&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;To reiterate: freelance is high risk and high reward.&lt;/p&gt;
&lt;p&gt;I personally find the autonomy liberating, and the variety of projects stimulating. On the other hand, I do miss some comforts of my former paid employment, as well as the type of high-context work that can only be done by long-term employees with specific knowledge.&lt;/p&gt;</content><category term="Business"/><category term="business"/><category term="freelance"/><category term="company"/></entry></feed>
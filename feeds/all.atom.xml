<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ömer Yüksel</title><link href="https://osyuksel.github.io/" rel="alternate"></link><link href="https://osyuksel.github.io/feeds/all.atom.xml" rel="self"></link><id>https://osyuksel.github.io/</id><updated>2025-06-13T15:20:00+02:00</updated><entry><title>Adversarial inputs in embedding spaces</title><link href="https://osyuksel.github.io/blog/embedding-spaces-adversarial/" rel="alternate"></link><published>2025-05-20T10:20:00+02:00</published><updated>2025-06-13T15:20:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-05-20:/blog/embedding-spaces-adversarial/</id><summary type="html">&lt;p&gt;Exploring embedding space vulnerabilities through lessons learned from a recent Kaggle competition.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Back when I was working on a Kaggle competition focused on &lt;a href="https://www.kaggle.com/competitions/llm-prompt-recovery"&gt;LLMs and text transformations&lt;/a&gt;, I discovered some fascinating patterns regarding embedding spaces and their edge-case behaviors when used for semantic similarity.&lt;/p&gt;
&lt;p&gt;The competition's goal was to guess the original prompt given to an LLM that converted text A to text B. Here, I'll focus on a particularly interesting aspect: how embedding spaces behave in unintuitive ways.&lt;/p&gt;
&lt;p&gt;Here's an example of what the competition data could look like:&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Original Text&lt;/th&gt;
&lt;th&gt;Prompt &lt;br&gt;&lt;em&gt;(Target - hidden during inference)&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;Transformed Text&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Beautiful is better than ugly.&lt;br&gt;Simple is better than complex.&lt;br&gt;Complex is better than complicated.&lt;/td&gt;
&lt;td&gt;Convert this text into a sea shanty&lt;/td&gt;
&lt;td&gt;Oh, beauty's the way, not ugliness bold,&lt;br&gt;And simplicity's path we shall hold!&lt;br&gt;Though complex she be, ain't complicated to see,&lt;br&gt;As we sail through these principles old!&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Not all those who wander are lost.&lt;br&gt;All that is gold does not glitter.&lt;/td&gt;
&lt;td&gt;Convert this text to Shakespearean style&lt;/td&gt;
&lt;td&gt;Hark! These wandering souls, they stray not from path divine,&lt;br&gt;Though golden treasures lack their expected shine.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Friends, Romans, countrymen, lend me your ears.&lt;br&gt;I come to bury Caesar, not to praise him.&lt;/td&gt;
&lt;td&gt;Convert this text to modern teen speech&lt;/td&gt;
&lt;td&gt;OMG guys! Everyone listen up!&lt;br&gt;I'm just here to get Caesar six feet under, not hype him up fr fr.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3 id="the-scoring-mechanism"&gt;The scoring mechanism&lt;/h3&gt;
&lt;p&gt;Comparing the similarity of two sentences – in this case, a candidate prompt and the actual prompt – is fundamentally challenging.&lt;/p&gt;
&lt;p&gt;The simplest approach, which is rewarding only exact matches, is too draconian. After all, there are often multiple valid ways to phrase the same instruction. String similarity metrics like Levenshtein distance aren't much better since they operate on surface-level text patterns: a one-letter difference can completely change the meaning of a sentence, while synonyms and rephrasing get heavily penalized despite preserving the semantic content.&lt;/p&gt;
&lt;p&gt;Bag of words and n-gram-based approaches suffer from similar issues. While they can capture some basic patterns of word usage, they still fail to recognize when two differently phrased prompts mean the same thing. They also tend to overemphasize common words and struggle with word order, which can be crucial for understanding meaning.&lt;/p&gt;
&lt;p&gt;Sentence embedding similarity seems like a more promising approach: in theory, it should capture the degree of semantic difference between prompts, understanding when two different phrasings carry the same meaning.&lt;/p&gt;
&lt;p&gt;This is the direction the competition took, using a &lt;a href="https://github.com/brohrer/sharpened-cosine-similarity/blob/main/README.md?plain=1#L32"&gt;sharpened cosine similarity&lt;/a&gt; with exponent of 3 between embeddings from the &lt;a href="https://huggingface.co/sentence-transformers/sentence-t5-base"&gt;T5 sentence&lt;/a&gt; model. The sharpening (cubing the cosine similarity) helps penalize wrong answers more strongly. In practice, however, the situation turns out to be more complex, as we'll see below.&lt;/p&gt;
&lt;p&gt;Let's look at similarity scores for candidate sentences compared to &lt;em&gt;Convert this text into a sea shanty.&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;sentence&lt;/th&gt;
&lt;th&gt;score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Transform this text into a sea shanty.&lt;/td&gt;
&lt;td&gt;0.970698&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into a shanty.&lt;/td&gt;
&lt;td&gt;0.901210&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into a song.&lt;/td&gt;
&lt;td&gt;0.638192&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Make this text better.&lt;/td&gt;
&lt;td&gt;0.621427&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into song lyrics.&lt;/td&gt;
&lt;td&gt;0.609144&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Improve this text.&lt;/td&gt;
&lt;td&gt;0.604455&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Convert this text into rap format.&lt;/td&gt;
&lt;td&gt;0.593446&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This scoring method turned out to be quite unforgiving. Missing a few key words could tank your score dramatically, even if you were conceptually in the right ballpark. For example, "Convert this text into a shanty" scores significantly lower than "Convert this text into a sea shanty," despite being semantically very similar. It gets worse if you manage to find the overall theme, song conversion but couldn't find the exact keyword, shanty.&lt;/p&gt;
&lt;h3 id="finding-the-mean-sentence"&gt;Finding the mean sentence&lt;/h3&gt;
&lt;p&gt;One interesting discovery came from analyzing how sentences relate to each other in the embedding space. By computing the average similarity between each sentence and all others in a diverse pool of prompts, we can find what I call a "mean sentence" – one that sits centrally in the embedding space.&lt;/p&gt;
&lt;p&gt;Vague and general instructions like "Make this text better" often score higher on average than more specific ones, even if sometimes, the specific sentence, according to a human reader, is more closely related to the original. This suggests that such generic prompts occupy a central position in the embedding space, making them potentially useful as fallback guesses when unsure about the specific prompt.&lt;/p&gt;
&lt;p&gt;These insights proved valuable – building up on this idea helped me eventually reach a high score. But the story doesn't end there. The most interesting findings (and ranking boost) came from pushing these observations further.&lt;/p&gt;
&lt;h3 id="exploring-edge-cases-in-embedding-space"&gt;Exploring edge cases in embedding space&lt;/h3&gt;
&lt;p&gt;Having found that vague instructions occupy a comparatively central position in the embedding space, two key questions emerge:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Can we find sentence embeddings that score even higher than generic phrases like "make this text better"?&lt;/li&gt;
&lt;li&gt;Do these optimal embeddings need to correspond to meaningful English sentences?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And the answers turn out to be interesting: there are unexpected patterns in how embedding spaces represent text. Even with the simplest optimization approach, we can generate nonsensical sequences that score higher on average than any natural language prompt in the embedding space.&lt;/p&gt;
&lt;p&gt;Here's a greedy algorithm that optimizes token selection to maximize embedding similarity scores. For each position from left to right, it tries every possible token and keeps the one that yields the highest average similarity against our sentence pool. After reaching the maximum length, it starts over from the beginning for additional optimization passes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;numpy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;generate_adversarial_prompt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sentence_pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Generate adversarial prompt for a given model and sentence pool.&lt;/span&gt;

&lt;span class="sd"&gt;   Intentionally left out early stopping to prevent getting stuck in a local minimum.&lt;/span&gt;
&lt;span class="sd"&gt;   &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

   &lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;
   &lt;span class="n"&gt;special_token_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;all_special_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;vocabulary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;token_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;token_id&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;special_token_ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;


   &lt;span class="n"&gt;current_tokens&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;

   &lt;span class="n"&gt;all_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
   &lt;span class="n"&gt;all_texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

   &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
     &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
         &lt;span class="n"&gt;candidates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
         &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
             &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;
             &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;        

         &lt;span class="n"&gt;candidate_texts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_tokens_to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

         &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sentence_pool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;candidate_texts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;current_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;next_token&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vocabulary&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
         &lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;next_token&lt;/span&gt;
         &lt;span class="n"&gt;current_text&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convert_tokens_to_string&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_tokens&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;current_text&lt;/span&gt;&lt;span class="si"&gt;=}&lt;/span&gt;&lt;span class="s2"&gt; &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;current_score&lt;/span&gt;&lt;span class="si"&gt;=}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

         &lt;span class="n"&gt;all_texts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
         &lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;all_texts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;(Full version is &lt;a href="https://gist.github.com/osyuksel/56f1efe3633c1682d3098aaf9467aa2f"&gt;here&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;length=10&lt;/code&gt; and &lt;code&gt;num_iterations=2&lt;/code&gt;, we can find a nonsensical string like "&lt;strong&gt;Text this PieceICWLISHTION aslucrarea&lt;/strong&gt;", which scores higher than any natural language prompt on average. This reveals a fundamental vulnerability in embedding-based scoring systems: they can be gamed by inputs that exploit the geometric properties of the embedding space without regard for semantic meaning.&lt;/p&gt;
&lt;p&gt;Performing beam search or using a more global optimization method like a genetic algorithm can help achieve an even higher score. In my experience, however, the returns were diminishing after the greedy search.&lt;/p&gt;
&lt;h3 id="related-work-and-interesting-token-phenomena"&gt;Related work and interesting token phenomena&lt;/h3&gt;
&lt;p&gt;The unexpected behaviors in embedding spaces aren't unique to this competition. One fascinating example is the case of &lt;a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"&gt;"SolidGoldMagikarp"&lt;/a&gt; – a now-patched token that became famous for causing unusual behaviors in GPT models. This seemingly random combination of words could cause language models to produce erratic outputs, demonstrating how certain token sequences can interact with embedding spaces in unexpected ways.&lt;/p&gt;
&lt;p&gt;Further investigations have revealed the existence of "under-trained tokens" in large language models, &lt;a href="https://medium.com/data-science/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec"&gt;as documented in this exploration&lt;/a&gt;. These are tokens that received limited exposure during training, potentially leading to unusual geometric positions in the embedding space. This relates to our findings where certain token combinations achieved unexpectedly high similarity scores despite lacking semantic meaning.&lt;/p&gt;
&lt;p&gt;A particularly interesting example comes from &lt;a href="https://aclanthology.org/D19-1221.pdf"&gt;research on universal adversarial triggers&lt;/a&gt;, where certain sequences of tokens can reliably cause specific behaviors in language models when prepended to any input. This demonstrates how the geometric properties of embedding spaces can be leveraged in systematic ways.&lt;/p&gt;
&lt;p&gt;The phenomenon isn't limited to text – similar patterns have been observed in &lt;a href="https://proceedings.neurips.cc/paper/2021/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html"&gt;multimodal embedding spaces&lt;/a&gt;, where researchers found that carefully crafted perturbations in one modality (like text) could affect the behavior of the system in another modality (like images).&lt;/p&gt;
&lt;h3 id="practical-implications"&gt;Practical implications&lt;/h3&gt;
&lt;p&gt;These findings have implications beyond just gaming a competition:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Scoring mechanisms based on embedding similarity might not be as robust as they appear at first glance.&lt;/li&gt;
&lt;li&gt;The relationship between semantic similarity and geometric proximity in embedding spaces isn't always intuitive.&lt;/li&gt;
&lt;li&gt;Systems using embedding similarity for matching or comparison might need additional safeguards against adversarial
   inputs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This observation becomes particularly relevant as more systems rely on embedding similarity for tasks like semantic
search, document comparison, or prompt matching. For instance, a malicious actor could potentially game search engine
rankings by generating content that is geometrically optimal in the embedding space but semantically meaningless to
humans. Embedding spaces are powerful tools, but their geometrical properties can sometimes be exploited in ways that
diverge from their intended semantic purpose.&lt;/p&gt;
&lt;p&gt;Intuitively, a more sophisticated model should be better at avoiding these pitfalls. For example, it could learn to distinguish between well-formed English sentences and nonsensical text, pushing gibberish away from the center of the embedding space. Models like OpenAI's embeddings, trained on vastly more data and with more parameters, might show more resistance to these attacks. However, I suspect this would only raise the bar for adversarial inputs rather than eliminate the fundamental vulnerability - the core problem of embedding spaces having a "middle ground" would likely remain.&lt;/p&gt;
&lt;p&gt;The challenge lies in developing scoring mechanisms that maintain the benefits of embedding spaces while being more resistant to these kinds of adversarial inputs. For this specific competition, a straightforward improvement would have been to combine the embedding similarity score with a binary classifier that detects valid English sentences. While this wouldn't completely prevent gaming the system, it would at least minimize nonsensical inputs like "&lt;strong&gt;Text this PieceICWLISHTION aslucrarea&lt;/strong&gt;" and force competitors to find adversarial inputs that either: (i) maintain grammatical structure or (ii) escape the classifier as well.&lt;/p&gt;
&lt;p&gt;Combining these insights about embedding space vulnerabilities with a fine-tuned model for prompt prediction, I managed to achieve &lt;a href="https://www.kaggle.com/competitions/llm-prompt-recovery/discussion/494569"&gt;11th place out of 2000+&lt;/a&gt; competitors in this Kaggle competition. The success came from understanding both the ML aspects of prompt recovery and the geometric properties of the scoring mechanism that could be exploited.&lt;/p&gt;
&lt;p&gt;This exploration into embedding spaces and their vulnerabilities was just one aspect of the competition, but it highlights broader challenges in building robust AI systems. Later, when building a client's text-based recommender system, these insights helped me better understand embedding limitations. As we continue to develop and deploy such systems, understanding these vulnerabilities becomes increasingly important.&lt;/p&gt;</content><category term="Technical"></category><category term="machine-learning"></category><category term="nlp"></category><category term="embeddings"></category></entry><entry><title>Going freelance in tech</title><link href="https://osyuksel.github.io/blog/freelancing-intro/" rel="alternate"></link><published>2025-05-01T10:20:00+02:00</published><updated>2025-05-20T20:53:00+02:00</updated><author><name>Ömer Yüksel</name></author><id>tag:osyuksel.github.io,2025-05-01:/blog/freelancing-intro/</id><summary type="html">&lt;p&gt;An introduction to tech freelancing with personal experiences from the Netherlands.&lt;/p&gt;</summary><content type="html">&lt;p&gt;It has been more than three years since I started freelancing, and although I'm relatively new, my experience so far might be helpful to those considering going freelance. Some of the things I write here are more specific to the Netherlands, but most can generalize to other parts of the world.&lt;/p&gt;
&lt;h3 id="freelance-vs-paid-employment"&gt;Freelance vs paid employment&lt;/h3&gt;
&lt;p&gt;For the majority of the IT freelancers, most of the income will likely be from long-running contract work, such as software development, data science, data engineering. The technical part of your daily work will remain similar to what you did before. In rarer cases, some succeed at selling their output rather than their time, such as with apps, SaaS, training, strategy consulting; however, these markets are more saturated.&lt;/p&gt;
&lt;p&gt;With that said, the advantages are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On average, a freelancer earns more than a paid employee.&lt;/li&gt;
&lt;li&gt;The autonomy – you are officially your own boss. The legal nature of your relationship with the company is a service provider/client one rather than employer/employee.&lt;/li&gt;
&lt;li&gt;You can experience different companies and projects during a relatively short time – your contracts are temporary by nature.&lt;/li&gt;
&lt;li&gt;You can choose your own budget for business training, conferences, etc. as a tax-deductible expense without requiring anyone's approval.&lt;/li&gt;
&lt;li&gt;Companies will typically expect you to "hit the ground running" and start solving their problems, so you can expect less fluff: fewer meetings, faster onboarding, and such.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Financial insecurity, both due to temporary contracts but also due to ease of termination.&lt;/li&gt;
&lt;li&gt;There is more paperwork to do. You will have to know some basics of taxation and administration.&lt;/li&gt;
&lt;li&gt;In a business-to-business contract, you are much more liable to damages – while it's covered by insurance, it can still be a daunting prospect. You also incur a greater risk of not getting paid for your work.&lt;/li&gt;
&lt;li&gt;Extra costs, such as accounting, business bank accounts, insurance. &lt;/li&gt;
&lt;li&gt;No extra benefits companies usually provide, like a pension, insurance, training budget, paid holidays, and sick days.&lt;/li&gt;
&lt;li&gt;A downside of "hitting the ground running" is that your projects may feel more detached from the business, and you may miss out on some interesting work that requires high business context.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I must reiterate the financial insecurity part – freelance is high risk, high reward, and I wouldn't recommend it to anyone without a significant financial buffer to live off from during dry periods. Having started towards the end of COVID and &lt;a href="https://en.wikipedia.org/wiki/Zero_interest-rate_policy"&gt;ZIRP&lt;/a&gt;, I experienced the impact of market swings in both directions. Freelancers may do much better than employees during a good economy, but they will also be the first ones to be affected by layoffs or pay decreases.&lt;/p&gt;
&lt;p&gt;In the specific case of the Netherlands, regulations regarding &lt;a href="https://www.kvk.nl/wetten-en-regels/wet-dba-voorkom-schijnzelfstandigheid/"&gt;Wet DBA&lt;/a&gt; (in Dutch) are also a factor, as it has scared off some companies from working with freelancers. I believe that with the right boundaries between you and the client, it is possible to add value while both retaining your autonomy and adhering to the regulations.&lt;/p&gt;
&lt;h3 id="company-structure"&gt;Company structure&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(Disclaimer: do not consider this section to be tax advice. Consult an accountant / tax specialist for your specific case.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You will typically have a choice between limited liability (a 'BV' in the Netherlands is typically the counterpart of American LLC) and sole proprietorship. With sole proprietorship, you receive the income from your work directly, whereas with the BV, you will be an employee of your own company and pay yourself a salary and a dividend for your earnings.&lt;/p&gt;
&lt;p&gt;In the Netherlands, the sole proprietorship has certain income tax advantages, that, up until a certain income, results in better net pay than a BV, especially in the first three years as a starting entrepreneur. Also, the paperwork with sole proprietorship is much simpler – if you speak Dutch and are not afraid of a bit of paperwork, you might even be able to get on without a bookkeeper.&lt;/p&gt;
&lt;p&gt;The disadvantage of the sole proprietorship is that your personal finances will be mixed up with your business finances. If you have solar panels in your house, for instance, tax authorities will start to treat this as if you have a mini power plant as a part of your business.&lt;/p&gt;
&lt;p&gt;A BV has a bigger administrative overhead, you will most definitely need an accountant not to mess up your administration, and you will end up paying both a corporate tax and a wage tax / dividend tax from your income. If you plan to reinvest your earnings into something later, like purchasing services, computing, etc., then this could be a good idea, because you defer the income tax until you pay yourself income. Otherwise, from a pure tax optimization point of view, the BV will only become profitable after a certain threshold. Keep in mind that you will have to pay yourself a minimum salary (which depends on a number of factors) before you can pay dividends. If your company doesn't make enough to cover your own salary for whatever reason, that may extra complicate the administration. Limited liability also makes the BV sound attractive regarding financial risks, but in the Netherlands the protection it provides seems quite limited.&lt;/p&gt;
&lt;p&gt;If there is a chance that you will scale up your company, sell it or partner up with someone in the future, produce intellectual property, etc., you can create a holding structure, which is typically a BV owning the other BV, which helps limit liability and separate your assets, at the cost of more administrative overhead.&lt;/p&gt;
&lt;p&gt;I personally found the simplest case to be less of a headache and decided to stick to sole proprietorship after briefly owning a BV.&lt;/p&gt;
&lt;h3 id="finding-clients"&gt;Finding clients&lt;/h3&gt;
&lt;p&gt;Ideally, you would have a network of your own clients that you directly do business with. In reality, building such a network will take time and patience, and until then, you will have to go for what is publicly available. I managed to have direct contracts with my clients around the second year, but I still keep in touch with the intermediaries to keep my options open.&lt;/p&gt;
&lt;h4 id="direct-contact"&gt;Direct contact&lt;/h4&gt;
&lt;p&gt;Some large organizations announce freelance assignments on their own portals. Especially for Dutch speakers, that might be a good way to have a direct contract with the client.&lt;/p&gt;
&lt;p&gt;Occasionally, other companies will announce over LinkedIn that they are looking for a freelancer. Having connections to the company's own recruiters might help spotting such opportunities.&lt;/p&gt;
&lt;h4 id="job-boards-and-intermediaries"&gt;Job boards and intermediaries&lt;/h4&gt;
&lt;p&gt;LinkedIn is a good starting point. There are also freelancer-specific boards such as &lt;a href="https://www.malt.nl/"&gt;Malt&lt;/a&gt; (international) and &lt;a href="https://mijn.freelance.nl/"&gt;Freelance.nl&lt;/a&gt; (Netherlands-only), gig-based platforms like Fiverr and Upwork, and intermediaries. &lt;/p&gt;
&lt;p&gt;About intermediaries, first there are platforms such as &lt;a href="https://freelancer.striive.com/"&gt;Striive&lt;/a&gt; or &lt;a href="https://portal.magnitglobal.com/"&gt;Magnit&lt;/a&gt;, which are typically used by government agencies and other large organizations. These platforms can be used by self-employed professionals, but also other intermediaries and employment agencies. They typically serve as marketplaces for assignments where you propose an hourly rate and your services.&lt;/p&gt;
&lt;p&gt;The second kind of intermediary, much more relevant to English speakers, is agencies. Sometimes they will post their assignments publicly, and other times they will reach out to those in their network privately first.&lt;/p&gt;
&lt;p&gt;In both cases, the intermediary makes money through a cut over your hourly rate. However, agencies take a greater cut than a marketplace does.&lt;/p&gt;
&lt;h4 id="more-about-agencies"&gt;More about agencies&lt;/h4&gt;
&lt;p&gt;Since most contracts for English speakers are through agencies, it makes sense to expand on this a bit.&lt;/p&gt;
&lt;p&gt;You will need to have a good agency to have the best deal you can get as a freelancer. A good agency will be transparent about their rates, only recommend you relevant assignments, give you tips to maximize your chances of getting the assignment, and offer a fair and reasonable contract.&lt;/p&gt;
&lt;p&gt;Some red flags to watch out for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Those offering already publicly available assignments (such as from the aforementioned marketplaces) that you can apply directly to.&lt;/li&gt;
&lt;li&gt;Outrageous profit margins. Always ask what their cut is and watch out for those that evade the question. More than 15 Eur/hour raises questions. You might think that what goes on between the agency and end-client is their own business, but keep in mind that the more the end-client pays, the more they will expect out of you. Therefore, a huge discrepancy between what they pay and what you receive may lead to soured relations.&lt;/li&gt;
&lt;li&gt;Unfair contract terms: think shifting all the liability to you, non-compete period over a year, asymmetric notice periods, not being allowed to cancel the contract.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, I had better experience with local and smaller firms, as they have more skin in the game and an incentive to build personal relationships.&lt;/p&gt;
&lt;p&gt;Another factor is whether the company allows building their recruiters a career or if they are burning through juniors: the ones in the latter category are more likely to see you as a row in their Excel sheet rather than a long-term business partner.&lt;/p&gt;
&lt;p&gt;You can make yourself available on LinkedIn and passively wait for agencies to contact you. However, in my limited experience, I had better luck actively applying for assignments that are out there rather than the passive approach – the latter turned out mostly bad fits.&lt;/p&gt;
&lt;h4 id="communication"&gt;Communication&lt;/h4&gt;
&lt;p&gt;Once you start your business, your number will be in a publicly accessible registry (Chamber of Commerce in the Netherlands). I highly recommend getting a business phone number in advance before creating your company. That is because this public number will likely receive many unwanted calls over time. In the Netherlands, you can expect spam calls trying to sell you an energy or internet contract.&lt;/p&gt;
&lt;p&gt;Most calls, however, will be from recruiters. In my experience, most of the projects from these calls were "shotgun" attempts and not very good fits – the same as outlined above about the passive approach. That's why I prioritize emails and LinkedIn messages over random calls as it allows filtering out bad matches from the start.&lt;/p&gt;
&lt;p&gt;However, your personal preference regarding calls may differ, and if you have the capacity, investing time in these calls can increase your chances of finding good opportunities.&lt;/p&gt;
&lt;p&gt;Regardless, it's important to remain polite and professional during these interactions, but also remember that your time is valuable. You do not owe fifteen minutes of your time to someone that has nothing meaningful to offer. Ultimately, &lt;a href="https://en.wikipedia.org/wiki/Sturgeon%27s_law"&gt;Sturgeon's Law&lt;/a&gt; will apply to many messages and cold calls you'll receive.&lt;/p&gt;
&lt;h3 id="determining-your-hourly-rate"&gt;Determining your hourly rate&lt;/h3&gt;
&lt;p&gt;It is best to ask around your own network, of course, but you can also find some publicly available information to have a reference point.&lt;/p&gt;
&lt;p&gt;Especially government agencies tend to be more transparent about the maximum budget, so that would provide a good place to start.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://techpays.com/"&gt;TechPays.com&lt;/a&gt;, a crowdsourced database primarily focused on employees, also has data on freelancers. Some freelance assignments on LinkedIn, as well as the aforementioned marketplaces, have visible hourly rate ranges.&lt;/p&gt;
&lt;p&gt;There is quite a variance in rates – if I had picked the first assignment offered to me, I would be earning half of what I make now. Location also matters – big cities tend to pay better than smaller ones, and in my experience, I saw a drop up to 40% for some cases in the Netherlands for the same title.&lt;/p&gt;
&lt;p&gt;Finally, for assignments through intermediaries, you can think of the process as a reverse auction: they sort offers from lowest to highest and pick the most affordable one that ticks all the boxes. Therefore, asking for the maximum budget will significantly lower your chances unless the project requires niche expertise.&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;To reiterate: freelance is high risk and high reward.&lt;/p&gt;
&lt;p&gt;I personally find the autonomy liberating, and the variety of projects stimulating. On the other hand, I do miss some comforts of my former paid employment, as well as the type of high-context work that can only be done by long-term employees with specific knowledge.&lt;/p&gt;</content><category term="Business"></category><category term="business"></category><category term="freelance"></category><category term="company"></category></entry></feed>